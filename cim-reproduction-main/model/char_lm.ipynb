{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7947953-dddd-4436-8f94-a78272d79d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom filtering and Damerau-Levenshtein functions loaded.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Character Language Model of CIM\n",
    "\n",
    "This notebook implements the character language model using the Hugging Face \n",
    "transformers’ implementations of BERT and BART. It includes:\n",
    "  - A custom top_k_top_p_filtering function (Option B)\n",
    "  - A custom Damerau–Levenshtein distance implementation (to remove dependency on the external package)\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import multiprocessing\n",
    "\n",
    "# Set a constant for maximum character positions\n",
    "DEFAULT_MAX_CHARACTER_POSITIONS = 64\n",
    "\n",
    "# Use a raw string for the character tokens to avoid escape sequence warnings.\n",
    "char_tokens = list(r\"0123456789abcdefghijklmnopqrstuvwxyz+-*/^.,;:=!?'()[]{}&\")\n",
    "special_tokens_fairseq = ['<s>', '<pad>', '</s>', '<unk>']\n",
    "special_tokens_bert = ['[CLS]', '[PAD]', '[SEP]', '[UNK]']  # Note: CLS and SEP do not exactly match.\n",
    "\n",
    "# --- Custom Filtering Function (Option B) ---\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\"), min_tokens_to_keep=1):\n",
    "    \"\"\"\n",
    "    Filters logits using top-k and nucleus (top-p) filtering.\n",
    "    This simple version sets logits outside the top k (and beyond cumulative top-p)\n",
    "    to filter_value.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid in-place modifications.\n",
    "    logits = logits.clone()\n",
    "    if top_k > 0:\n",
    "        topk_values, _ = torch.topk(logits, top_k)\n",
    "        kth_value = topk_values[..., -1, None]\n",
    "        logits = torch.where(logits < kth_value, torch.full_like(logits, filter_value), logits)\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "# --- Custom Damerau–Levenshtein Implementation ---\n",
    "def damerauLevenshtein(seq1, seq2, similarity=False):\n",
    "    \"\"\"\n",
    "    Compute the Damerau–Levenshtein distance between two sequences.\n",
    "    This implementation supports insertion, deletion, substitution, and adjacent transpositions.\n",
    "    The 'similarity' parameter is ignored in this version.\n",
    "    \"\"\"\n",
    "    len1 = len(seq1)\n",
    "    len2 = len(seq2)\n",
    "    d = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n",
    "    for i in range(len1 + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        d[0][j] = j\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            d[i][j] = min(\n",
    "                d[i - 1][j] + 1,         # deletion\n",
    "                d[i][j - 1] + 1,         # insertion\n",
    "                d[i - 1][j - 1] + cost   # substitution\n",
    "            )\n",
    "            if i > 1 and j > 1 and seq1[i - 1] == seq2[j - 2] and seq1[i - 2] == seq2[j - 1]:\n",
    "                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)  # transposition\n",
    "    return d[len1][len2]\n",
    "\n",
    "print(\"Custom filtering and Damerau-Levenshtein functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "907702e6-1555-4b02-8cba-cdb5f23e089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Classes and Model Definitions #\n",
    "#################################\n",
    "\n",
    "class BeamHypotheses(object):\n",
    "    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length - 1  # ignoring BOS token\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.num_beams = num_beams\n",
    "        self.beams = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.beams)\n",
    "\n",
    "    def add(self, hyp, sum_logprobs, sum_logprobs2=0.0):\n",
    "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
    "        score2 = sum_logprobs2 / len(hyp) ** self.length_penalty\n",
    "        if len(self) < self.num_beams or score > self.worst_score:\n",
    "            self.beams.append((score, hyp, score2))\n",
    "            if len(self) > self.num_beams:\n",
    "                sorted_scores = sorted([(s, idx) for idx, (s, _, _) in enumerate(self.beams)])\n",
    "                del self.beams[sorted_scores[0][1]]\n",
    "                self.worst_score = sorted_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs, cur_len):\n",
    "        if len(self) < self.num_beams:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            cur_score = best_sum_logprobs / cur_len ** self.length_penalty\n",
    "            return self.worst_score >= cur_score\n",
    "\n",
    "# Note: We now subclass BartPreTrainedModel (as recommended) instead of the deprecated PretrainedBartModel.\n",
    "class CharacterLanguageModel(transformers.BartPreTrainedModel):\n",
    "    def __init__(self, args, config, bert_model, char_decoder):\n",
    "        super().__init__(config)\n",
    "        self.args = args\n",
    "        self.config = config\n",
    "        self.final_logits_bias = torch.nn.parameter.Parameter(torch.zeros(1, self.config.vocab_size))\n",
    "        self.encoder = bert_model.bert\n",
    "        self.decoder = char_decoder\n",
    "        self.train_bert = args.train_bert\n",
    "        if not self.train_bert:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.bos, self.pad, self.eos, self.unk = special_tokens_fairseq\n",
    "        self.ed_pool_master = multiprocessing.pool.ThreadPool(processes=1)\n",
    "        self.ed_pool_worker = multiprocessing.Pool(processes=4)\n",
    "\n",
    "    @classmethod\n",
    "    def get_bert_config(cls, bert_config_file):\n",
    "        with open(bert_config_file, 'r') as fd:\n",
    "            config_orig = json.load(fd)\n",
    "        bert_config = transformers.BertConfig(\n",
    "            vocab_size=config_orig['vocab_size'],\n",
    "            hidden_size=config_orig['hidden_size'],\n",
    "            num_hidden_layers=config_orig['num_hidden_layers'],\n",
    "            num_attention_heads=config_orig['num_attention_heads'],\n",
    "            intermediate_size=config_orig['intermediate_size'],\n",
    "            hidden_act=config_orig['hidden_act'],\n",
    "            hidden_dropout_prob=config_orig['hidden_dropout_prob'],\n",
    "            attention_probs_dropout_prob=config_orig['attention_probs_dropout_prob'],\n",
    "            max_position_embeddings=config_orig['max_position_embeddings'],\n",
    "            type_vocab_size=config_orig['type_vocab_size'],\n",
    "            initializer_range=config_orig['initializer_range'],\n",
    "            layer_norm_eps=1e-12\n",
    "        )\n",
    "        return bert_config\n",
    "\n",
    "    @classmethod\n",
    "    def build_bert_model(cls, bert_config):\n",
    "        bert_model = transformers.BertForPreTraining(bert_config)\n",
    "        return bert_model\n",
    "\n",
    "    @classmethod\n",
    "    def get_bert_tokenizer(cls, bert_vocab_file):\n",
    "        bert_tokenizer = transformers.BertTokenizer(\n",
    "            bert_vocab_file,\n",
    "            do_lower_case=True,\n",
    "            do_basic_tokenize=True\n",
    "        )\n",
    "        return bert_tokenizer\n",
    "\n",
    "    @classmethod\n",
    "    def get_char_embeddings_from_bert(cls, bert_embeddings, bert_tokenizer):\n",
    "        char_word_ids = []\n",
    "        char_word_ids.append(bert_tokenizer.cls_token_id)  # CLS -> BOS\n",
    "        char_word_ids.append(bert_tokenizer.pad_token_id)\n",
    "        char_word_ids.append(bert_tokenizer.sep_token_id)  # SEP -> EOS\n",
    "        char_word_ids.append(bert_tokenizer.unk_token_id)\n",
    "        char_word_ids.extend(bert_tokenizer.convert_tokens_to_ids(char_tokens))\n",
    "        if isinstance(bert_embeddings, torch.nn.modules.sparse.Embedding):\n",
    "            embedding_matrix = bert_embeddings(torch.tensor(char_word_ids)).detach()\n",
    "        elif isinstance(bert_embeddings, torch.Tensor):\n",
    "            embedding_matrix = bert_embeddings[char_word_ids, :].detach()\n",
    "        char_embeddings = torch.nn.Embedding.from_pretrained(embedding_matrix,\n",
    "                                freeze=False, padding_idx=1)\n",
    "        return char_embeddings\n",
    "\n",
    "    @classmethod\n",
    "    def get_char_decoder_config(cls, bert_config_file, args):\n",
    "        with open(bert_config_file, 'r') as fd:\n",
    "            config_orig = json.load(fd)\n",
    "        bart_config = transformers.BartConfig(\n",
    "            is_decoder=True,\n",
    "            vocab_size=len(char_tokens) + len(special_tokens_fairseq),\n",
    "            d_model=config_orig['hidden_size'],\n",
    "            decoder_layers=args.decoder_layers,\n",
    "            max_position_embeddings=DEFAULT_MAX_CHARACTER_POSITIONS,\n",
    "            init_str=config_orig['initializer_range']\n",
    "        )\n",
    "        return bart_config\n",
    "\n",
    "    @classmethod\n",
    "    def build_char_decoder(cls, bart_config, char_embedding):\n",
    "        \"\"\"Create BART decoder for character LM.\"\"\"\n",
    "        from transformers.models.bart.modeling_bart import BartDecoder  # Updated module path\n",
    "        bart_decoder = BartDecoder(bart_config, char_embedding)\n",
    "        return bart_decoder\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids_context,\n",
    "                attention_mask_context=None,\n",
    "                input_ids_correct=None,\n",
    "                attention_mask_correct=None,\n",
    "                target_correct=None,\n",
    "                encoder_outputs=None,\n",
    "                encoder_embeds=None,\n",
    "                past_key_values=None,\n",
    "                use_cache=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,\n",
    "                **kwargs):\n",
    "        if encoder_embeds is not None:\n",
    "            encoder_outputs = (encoder_embeds,)\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids_context,\n",
    "                attention_mask=attention_mask_context,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        # Remove EOS (token id 2) from correct input by converting them to PAD (token id 1)\n",
    "        decoder_input_ids = input_ids_correct - input_ids_correct.eq(2).int()\n",
    "        if attention_mask_correct is not None:\n",
    "            decoder_padding_mask = attention_mask_correct.eq(0) | input_ids_correct.eq(2)\n",
    "        else:\n",
    "            decoder_padding_mask = decoder_input_ids.eq(1)\n",
    "        _, _, causal_mask = transformers.modeling_bart._prepare_bart_decoder_inputs(\n",
    "            self.config,\n",
    "            input_ids=input_ids_correct,\n",
    "            decoder_input_ids=None,\n",
    "            decoder_padding_mask=attention_mask_correct,\n",
    "            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype,\n",
    "        )\n",
    "        decoder_outputs = self.decoder(\n",
    "            decoder_input_ids,\n",
    "            encoder_outputs[0],\n",
    "            attention_mask_context,\n",
    "            decoder_padding_mask,\n",
    "            decoder_causal_mask=causal_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        lm_output_logits = F.linear(decoder_outputs[0], self.decoder.embed_tokens.weight,\n",
    "                                    bias=self.final_logits_bias)\n",
    "        output_logits = transformers.modeling_outputs.Seq2SeqLMOutput(\n",
    "            logits=lm_output_logits,\n",
    "        ).logits\n",
    "\n",
    "        if not kwargs.get('return_encoder_embeds'):\n",
    "            return output_logits\n",
    "        else:\n",
    "            return encoder_outputs[0], output_logits\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        from transformers.models.bart.modeling_bart import _make_linear_from_emb\n",
    "        return _make_linear_from_emb(self.decoder.embed_tokens)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, decoder_input_ids, past=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs):\n",
    "        return {\n",
    "            \"input_ids_context\": None,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": past,\n",
    "            \"input_ids_correct\": decoder_input_ids,\n",
    "            \"attention_mask_context\": attention_mask,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n",
    "        if cur_len == 1 and self.config.force_bos_token_to_be_generated:\n",
    "            self._force_token_id_to_be_generated(logits, self.config.bos_token_id)\n",
    "        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:\n",
    "            self._force_token_id_to_be_generated(logits, self.config.eos_token_id)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def _force_token_id_to_be_generated(scores, token_id) -> None:\n",
    "        scores[:, [x for x in range(scores.shape[1]) if x != token_id]] = -float(\"inf\")\n",
    "\n",
    "    def parameters(self):\n",
    "        if len(list(super().parameters())):\n",
    "            ret = super().parameters()\n",
    "        elif hasattr(self, \"_former_parameters\"):\n",
    "            ret = (p for m in self.modules() for p in m._former_parameters.values())\n",
    "        elif hasattr(self, \"_parameters\"):\n",
    "            ret = (p for m in self.modules() for p in m._parameters.values())\n",
    "        return ret\n",
    "\n",
    "    def _compute_edit_distances(self, input_ids_list, typo_token_ids_strip, vocab_size, edit_distance_extra_len):\n",
    "        batch_size = len(typo_token_ids_strip)\n",
    "        num_beams = len(input_ids_list) // batch_size\n",
    "        cur_len = len(input_ids_list[0])\n",
    "        dataset = [\n",
    "            (input_ids_list[i * num_beams : (i + 1) * num_beams],\n",
    "             typo_token_ids_strip[i],\n",
    "             vocab_size,\n",
    "             edit_distance_extra_len)\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "        num_process = min(batch_size, 4)\n",
    "        edit_distance_mat = list(self.ed_pool_worker.imap(_edit_distance_pool_job, dataset, chunksize=len(dataset) // num_process))\n",
    "        return edit_distance_mat\n",
    "\n",
    "    def get_param_group(self, finetune_bert=False, finetune_factor=0.1):\n",
    "        if not finetune_bert:\n",
    "            param_group = [{'params': list(self.parameters()), 'lr_factor': 1.0}]\n",
    "        else:\n",
    "            bert_params = list(self.encoder.parameters())\n",
    "            bert_params_set = set(bert_params)\n",
    "            other_params = [p for p in list(self.parameters()) if p not in bert_params_set]\n",
    "            param_group = [{'params': other_params, 'lr_factor': 1.0},\n",
    "                           {'params': bert_params, 'lr_factor': finetune_factor}]\n",
    "        return param_group\n",
    "\n",
    "    @staticmethod\n",
    "    def get_set_lr():\n",
    "        def set_lr(self, lr):\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                if 'lr_factor' in param_group:\n",
    "                    param_group['lr'] = lr * param_group['lr_factor']\n",
    "                else:\n",
    "                    param_group['lr'] = lr\n",
    "        return set_lr\n",
    "\n",
    "def _edit_distance_pool_job(data):\n",
    "    input_ids_ex, typo_token_ids_strip_ex, vocab_size, edit_distance_extra_len = data\n",
    "    num_beams, cur_len = len(input_ids_ex), len(input_ids_ex[0])\n",
    "    ed_submat = [[0.0 for _ in range(vocab_size)] for _ in range(num_beams)]\n",
    "    # Use our custom damerauLevenshtein function defined above.\n",
    "    typo_token_ids_temp = typo_token_ids_strip_ex[: cur_len + 1 + edit_distance_extra_len]\n",
    "    for idx1 in range(num_beams):\n",
    "        temp = input_ids_ex[idx1] + [0]\n",
    "        for idx2 in range(vocab_size):\n",
    "            temp[-1] = idx2\n",
    "            ed_submat[idx1][idx2] = damerauLevenshtein(temp, typo_token_ids_temp, similarity=False)\n",
    "    return ed_submat\n",
    "\n",
    "def _trie_score_pool_job(input_ids_ex, trie, vocab_size):\n",
    "    num_examples, cur_len = len(input_ids_ex), len(input_ids_ex[0])\n",
    "    score = [[float('-inf') for _ in range(vocab_size)] for _ in range(num_examples)]\n",
    "    for i, prefix_ids in enumerate(input_ids_ex):\n",
    "        cand_ids = trie.get_candidate_chars(prefix_ids[1:])  # ignore BOS token\n",
    "        for j in cand_ids:\n",
    "            score[i][j] = 0.0\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d78f5e-647c-490e-b877-aa0775f7ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer(object):\n",
    "    def __init__(self, max_length=DEFAULT_MAX_CHARACTER_POSITIONS):\n",
    "        self.max_length = max_length\n",
    "        self.bos, self.pad, self.eos, self.unk = special_tokens_fairseq\n",
    "        self.bos_index, self.pad_index, self.eos_index, self.unk_index = 0, 1, 2, 3\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        for i, c in enumerate(special_tokens_fairseq + char_tokens):\n",
    "            self.char_to_id[c] = i\n",
    "            self.id_to_char[i] = c\n",
    "\n",
    "    def tokenize(self, text, eos_bos=True, padding_end=False, max_length=None, output_token_ids=False):\n",
    "        assert isinstance(text, str)\n",
    "        max_seq_len = self.max_length - 2 if eos_bos else self.max_length\n",
    "        tokens = []\n",
    "        attention_mask = []\n",
    "        for c in text[:max_seq_len]:\n",
    "            if c in self.char_to_id:\n",
    "                tokens.append(c)\n",
    "            else:\n",
    "                tokens.append(self.unk)\n",
    "            attention_mask.append(1)\n",
    "        if eos_bos:\n",
    "            tokens.insert(0, '<s>')\n",
    "            tokens.append(self.eos)\n",
    "            attention_mask.insert(0, 1)\n",
    "            attention_mask.append(1)\n",
    "        if padding_end:\n",
    "            max_length = max_length if max_length is not None else self.max_length\n",
    "            while len(tokens) < max_length:\n",
    "                tokens.append(self.pad)\n",
    "                attention_mask.append(0)\n",
    "        if output_token_ids:\n",
    "            return self.convert_tokens_to_ids(tokens), attention_mask\n",
    "        else:\n",
    "            return tokens, attention_mask\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.char_to_id[t] for t in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.cpu().detach().tolist()\n",
    "        return [self.id_to_char[i] for i in ids]\n",
    "\n",
    "class SmoothCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmoothCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, target, smoothing=0.0):\n",
    "        if target.dim() == logits.dim() - 1:\n",
    "            target = target.unsqueeze(-1)\n",
    "        lprobs = F.log_softmax(logits, dim=-1)\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=target).squeeze(-1)\n",
    "        smooth_loss = -lprobs.sum(dim=-1)\n",
    "        eps_i = smoothing / lprobs.size(-1)\n",
    "        loss = (1.0 - smoothing) * nll_loss + eps_i * smooth_loss\n",
    "        return loss, nll_loss\n",
    "\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets=None, target_mask=None):\n",
    "        if targets.dim() == logits.dim() - 1:\n",
    "            targets = targets.unsqueeze(-1)\n",
    "        lprobs = F.log_softmax(logits, dim=-1)\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=targets).squeeze(-1)\n",
    "        if target_mask is not None:\n",
    "            loss = ((nll_loss * target_mask).sum(-1) / target_mask.sum(-1)).mean()\n",
    "        else:\n",
    "            loss = nll_loss.mean()\n",
    "        return loss\n",
    "\n",
    "class Trie(object):\n",
    "    def __init__(self, char_tokenizer, eos_token_id=2):\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self._trie = {}\n",
    "\n",
    "    def add_word_ids(self, word_token_ids):\n",
    "        trie = self._trie\n",
    "        for token_id in word_token_ids:\n",
    "            if token_id not in trie:\n",
    "                trie[token_id] = {}\n",
    "            trie = trie[token_id]\n",
    "        trie[self.eos_token_id] = {}\n",
    "\n",
    "    def add_words(self, words):\n",
    "        for word in words:\n",
    "            try:\n",
    "                word_token_ids = self.char_tokenizer.convert_tokens_to_ids(word)\n",
    "                self.add_word_ids(word_token_ids)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    def get_candidate_chars(self, prefix_ids):\n",
    "        trie = self._trie\n",
    "        for token_id in prefix_ids:\n",
    "            if token_id not in trie:\n",
    "                return []\n",
    "            trie = trie[token_id]\n",
    "        return trie.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60bf8deb-1a4b-42ef-bab2-8c431b93af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT configuration loaded successfully.\n",
      "BERT model built successfully.\n",
      "BERT tokenizer created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example: Initialize the model.\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Define your parameters inline.\n",
    "args = SimpleNamespace(\n",
    "    mimic_csv_dir=\"data/mimic3/split\",\n",
    "    data_dir=\"data/mimic_synthetic\",\n",
    "    dict_file=\"data/lexicon/lexicon_en.json\",\n",
    "    bert_dir=\"bert/ncbi_bert_base\",\n",
    "    output_dir=\"results/cim_base\",\n",
    "    is_train=True,\n",
    "    test_file=\"test.tsv\",\n",
    "    init_ckpt=None,\n",
    "    init_step=0,\n",
    "    seed=123,\n",
    "    decoder_layers=12,\n",
    "    train_bert=True,\n",
    "    bert_finetune_factor=1.0,\n",
    "    dropout=0.1,\n",
    "    synthetic_min_word_len=3,\n",
    "    do_substitution=True,\n",
    "    do_transposition=True,\n",
    "    max_word_corruptions=2,\n",
    "    no_corruption_prob=0.0,\n",
    "    train_with_ed=False,\n",
    "    batch_size=256,\n",
    "    num_gpus=4,\n",
    "    optimizer='adam',\n",
    "    adam_betas=\"(0.9, 0.999)\",\n",
    "    adam_eps=1e-08,\n",
    "    weight_decay=0.01,\n",
    "    training_step=500000,\n",
    "    display_iter=100,\n",
    "    eval_iter=25000,\n",
    "    lr=[0.0001],\n",
    "    warmup_updates=10000,\n",
    "    warmup_init_lr=1e-6,\n",
    "    num_beams=3,\n",
    "    edit_distance_weight=5.0,\n",
    "    edit_distance_extra_len=100,\n",
    "    length_penalty=1.0,\n",
    "    beam_sort_linear_ed=False,\n",
    "    beam_final_score_normalize_ed=False,\n",
    "    dict_matching=True\n",
    ")\n",
    "\n",
    "# Set absolute paths for BERT configuration and vocabulary files\n",
    "bert_config_file = r\"C:\\Users\\chen5\\Desktop\\FinalP1\\cim-misspelling-main\\bert\\ncbi_bert_base\\bert_config.json\"\n",
    "bert_vocab_file = r\"C:\\Users\\chen5\\Desktop\\FinalP1\\cim-misspelling-main\\bert\\ncbi_bert_base\\vocab.txt\"\n",
    "\n",
    "# Try loading the configuration and initializing the BERT model and its tokenizer\n",
    "try:\n",
    "    # Load configuration from the JSON file\n",
    "    bert_config = CharacterLanguageModel.get_bert_config(bert_config_file)\n",
    "    print(\"BERT configuration loaded successfully.\")\n",
    "\n",
    "    # Build the BERT model using the configuration\n",
    "    bert_model = CharacterLanguageModel.build_bert_model(bert_config)\n",
    "    print(\"BERT model built successfully.\")\n",
    "\n",
    "    # Create the tokenizer from the vocabulary file\n",
    "    bert_tokenizer = CharacterLanguageModel.get_bert_tokenizer(bert_vocab_file)\n",
    "    print(\"BERT tokenizer created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error initializing BERT components:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89eb6ae-6e37-424c-8fc6-08926f66a7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
