{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a91b218-9f01-4d0f-9382-94ca165f9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from utils import clean_text, sanitize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1bc093f-5e64-4d71-8bb9-d7554561b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "mimic_note_fpath = '../data/mimic3/NOTEEVENTS.csv'        # MIMIC-III clinical notes\n",
    "mimic_tools_dpath = '../scripts/mimic-tools/'              # Directory containing pseudonymization scripts\n",
    "lexicon_fpath = '../data/lexicon/lexicon.json'             # Lexicon dictionary\n",
    "\n",
    "# Output paths and configuration\n",
    "data_root = '../data/mimic_synthetic/'\n",
    "num_val_examples = 10000\n",
    "num_test_examples = 10000\n",
    "num_examples = num_val_examples + num_test_examples\n",
    "all_output_fpath = os.path.join(data_root, 'all.tsv')\n",
    "val_output_fpath = os.path.join(data_root, 'val.tsv')\n",
    "test_output_fpath = os.path.join(data_root, 'test.tsv')\n",
    "\n",
    "# Word filtering/corruption parameters\n",
    "min_word_len = 1  # Accept all words with at least 1 character \n",
    "no_corruption_prob = 0.1\n",
    "max_corruptions = 2\n",
    "do_substitution = True\n",
    "do_transposition = True\n",
    "DEFAULT_MAX_CHARACTER_POSITIONS = 64\n",
    "\n",
    "# Directories for temporary pseudonymization\n",
    "pseudo_in_dpath = os.path.join(data_root, 'temp')\n",
    "pseudo_out_dpath = os.path.join(data_root, 'temp_pseudonym')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecccf3f7-e5a5-465f-80cf-6e228e2fc7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NOTEEVENTS.csv... done! 2083180 notes\n",
      "Reading ../data/lexicon/lexicon.json... 822919 words\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADING\n",
    "# ================================\n",
    "# Load MIMIC-III notes CSV file\n",
    "print(f\"Reading {os.path.basename(mimic_note_fpath)}... \", end=\"\")\n",
    "df_notes = pd.read_csv(mimic_note_fpath, low_memory=False)\n",
    "df_notes = df_notes.set_index('ROW_ID')\n",
    "print(f\"done! {len(df_notes)} notes\")\n",
    "\n",
    "# Load lexicon for word validation\n",
    "print(f\"Reading {lexicon_fpath}... \", end=\"\")\n",
    "with open(lexicon_fpath, 'r', encoding='utf-8') as fd:\n",
    "    vocab = json.load(fd)\n",
    "vocab_set = set(vocab)\n",
    "print(f\"{len(vocab)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ce98ac-f66a-4e95-b147-7ee240d0b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "# ================================\n",
    "\n",
    "# Function: select a random valid word from a clinical note and return its context\n",
    "puncs = list(\"[]!\\\"#$%&'()*+,./:;<=>?@\\\\^_`{|}~-\")\n",
    "def random_word_context(text, max_trial=100):\n",
    "    words = text.split()\n",
    "    trial = 0\n",
    "    while trial < max_trial:\n",
    "        trial += 1\n",
    "        w_idx = random.randint(0, len(words)-1)\n",
    "        word = words[w_idx]\n",
    "        left_res = []\n",
    "        right_res = []\n",
    "        # First try: if the word is directly acceptable\n",
    "        if len(word) >= min_word_len and word.lower() in vocab_set and len(word) < DEFAULT_MAX_CHARACTER_POSITIONS - 4:\n",
    "            return word, ' '.join(words[:w_idx]), ' '.join(words[w_idx+1:])\n",
    "        else:\n",
    "            # Remove punctuation from beginning and end\n",
    "            if word and word[0] in puncs:\n",
    "                left_res = [word[0]]\n",
    "                word = word[1:]\n",
    "            if not word:\n",
    "                continue\n",
    "            if word and word[-1] in puncs:\n",
    "                right_res = [word[-1]]\n",
    "                word = word[:-1]\n",
    "            if len(word) < min_word_len or word.lower() not in vocab_set or len(word) >= DEFAULT_MAX_CHARACTER_POSITIONS - 4:\n",
    "                continue\n",
    "            # Check for anonymized fields in surrounding context \n",
    "            right_snip = ' '.join(words[w_idx+1:w_idx+5])\n",
    "            left_snip = ' '.join(words[max(0, w_idx-4):w_idx])\n",
    "            if ('**]' in right_snip and '[**' not in right_snip) or ('[**' in left_snip and '**]' not in left_snip):\n",
    "                continue\n",
    "            return word, ' '.join(words[:w_idx] + left_res), ' '.join(right_res + words[w_idx+1:])\n",
    "    raise ValueError(\"Failed to choose a valid word context.\")\n",
    "\n",
    "# Functions to perform character-level corruption\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def random_alphabet():\n",
    "    return random.choice(alphabet)\n",
    "\n",
    "# Build list of possible operations based on configuration\n",
    "operation_list = ['ins', 'del']\n",
    "if do_substitution:\n",
    "    operation_list.append('sub')\n",
    "if do_transposition:\n",
    "    operation_list.append('tra')\n",
    "\n",
    "def single_corruption(word):\n",
    "    while True:\n",
    "        oper = random.choice(operation_list)\n",
    "        if oper == \"del\":  \n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            cidx = random.randint(0, len(word)-1)\n",
    "            ret = word[:cidx] + word[cidx+1:]\n",
    "            break\n",
    "        elif oper == \"ins\":  \n",
    "            cidx = random.randint(0, len(word))\n",
    "            ret = word[:cidx] + random_alphabet() + word[cidx:]\n",
    "            break\n",
    "        elif oper == \"sub\":  \n",
    "            cidx = random.randint(0, len(word)-1)\n",
    "            new_char = random_alphabet()\n",
    "            while new_char == word[cidx]:\n",
    "                new_char = random_alphabet()\n",
    "            ret = word[:cidx] + new_char + word[cidx+1:]\n",
    "            break\n",
    "        elif oper == \"tra\": \n",
    "            if len(word) == 1:\n",
    "                continue\n",
    "            cidx = random.randint(0, len(word)-2)\n",
    "            if word[cidx] == word[cidx+1]:\n",
    "                continue\n",
    "            ret = word[:cidx] + word[cidx+1] + word[cidx] + word[cidx+2:]\n",
    "            break\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown operation: {oper}\")\n",
    "    return ret\n",
    "\n",
    "def corrupt_word(word_original, max_corruptions=max_corruptions):\n",
    "    # With a certain probability, leave the word unmodified.\n",
    "    if no_corruption_prob > 0.0 and random.uniform(0, 1) < no_corruption_prob:\n",
    "        return word_original\n",
    "    num_corruptions = random.randint(1, max_corruptions)\n",
    "    # Repeat corruption until the corrupted word is different from the original.\n",
    "    corrupted = word_original\n",
    "    while True:\n",
    "        temp_word = corrupted\n",
    "        for i in range(num_corruptions):\n",
    "            temp_word = single_corruption(temp_word)\n",
    "        if temp_word != word_original:\n",
    "            corrupted = temp_word\n",
    "            break\n",
    "    return corrupted\n",
    "\n",
    "# Process note text (cleaning and sanitizing)\n",
    "def process_note(note):\n",
    "    note = re.sub('\\n', ' ', note)\n",
    "    note = re.sub('\\t', ' ', note)\n",
    "    return sanitize_text(clean_text(note))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82b9d92e-7585-4d6f-98a4-57e8b21c7b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected note IDs (first 10): [655360, 393217, 524290, 393216, 393224, 16, 393244, 1572897, 34, 786477]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting word contexts: 100%|██████████| 20000/20000 [00:03<00:00, 5998.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 words have punctuation\n",
      "Starting pseudonymization with command:\n",
      "python ../scripts/mimic-tools/main.py REPLACE --input-dir C:\\Users\\chen5\\Desktop\\FinalP2\\cim-misspelling-main\\scripts\\../data/mimic_synthetic/temp --output-dir C:\\Users\\chen5\\Desktop\\FinalP2\\cim-misspelling-main\\scripts\\../data/mimic_synthetic/temp_pseudonym --list-dir ../scripts/mimic-tools/lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pseudonymized notes: 100%|██████████| 20000/20000 [12:31<00:00, 26.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating corrupted words...\n",
      "\tbp -> dcbp\n",
      "\ttracking -> ztracking\n",
      "\tnot -> not\n",
      "\tmuch -> uwmch\n",
      "\tto -> ot\n",
      "\tpatient -> patient\n",
      "Done generating corrupted words!\n"
     ]
    }
   ],
   "source": [
    "# MAIN PROCEDURE\n",
    "# ================================\n",
    "\n",
    "# 1. Randomly select note ids that satisfy the text length requirement.\n",
    "random.seed(1234)\n",
    "note_ids = list(df_notes.index)\n",
    "random.shuffle(note_ids)\n",
    "typo_noteids = set()\n",
    "selected_count = 0\n",
    "for nid in note_ids:\n",
    "    note = str(df_notes.loc[nid].TEXT)  # Ensure note text is a string.\n",
    "    if len(note.strip()) >= 2000 and nid not in typo_noteids:\n",
    "        typo_noteids.add(nid)\n",
    "        selected_count += 1\n",
    "    if selected_count == num_examples:\n",
    "        break\n",
    "typo_noteids = list(typo_noteids)\n",
    "print(\"Selected note IDs (first 10):\", typo_noteids[:10])\n",
    "\n",
    "# 2. For each selected note, pick a random word with context.\n",
    "examples = []\n",
    "for nid in tqdm(typo_noteids, desc=\"Extracting word contexts\"):\n",
    "    note = str(df_notes.loc[nid].TEXT)\n",
    "    try:\n",
    "        word, left, right = random_word_context(note)\n",
    "        examples.append([word, left, right])\n",
    "    except Exception as e:\n",
    "        # If no valid word is found, use empty strings.\n",
    "        examples.append([\"\", \"\", \"\"])\n",
    "\n",
    "# Debug: Check how many chosen words contain punctuation.\n",
    "words = [ex[0] for ex in examples]\n",
    "words_with_punc = [w for w in words if any(not c.isalpha() for c in w)]\n",
    "print(f\"{len(words_with_punc)} words have punctuation\")\n",
    "# print(words_with_punc)  \n",
    "\n",
    "# 3. Write out left/right contexts to disk for pseudonymization.\n",
    "if os.path.exists(pseudo_in_dpath):\n",
    "    shutil.rmtree(pseudo_in_dpath)\n",
    "if os.path.exists(pseudo_out_dpath):\n",
    "    shutil.rmtree(pseudo_out_dpath)\n",
    "os.makedirs(pseudo_in_dpath, exist_ok=True)\n",
    "for noteid, example in zip(typo_noteids, examples):\n",
    "    left_text = example[1]\n",
    "    right_text = example[2]\n",
    "    with open(os.path.join(pseudo_in_dpath, f'{noteid}_left.txt'), 'w', encoding='utf-8') as fd:\n",
    "        fd.write(left_text)\n",
    "    with open(os.path.join(pseudo_in_dpath, f'{noteid}_right.txt'), 'w', encoding='utf-8') as fd:\n",
    "        fd.write(right_text)\n",
    "\n",
    "# 4. Run the external pseudonymization script \n",
    "cmd = f\"python {os.path.join(mimic_tools_dpath, 'main.py')} REPLACE \" \\\n",
    "      f\"--input-dir {os.path.join(os.getcwd(), pseudo_in_dpath)} \" \\\n",
    "      f\"--output-dir {os.path.join(os.getcwd(), pseudo_out_dpath)} \" \\\n",
    "      f\"--list-dir {os.path.join(mimic_tools_dpath, 'lists')}\"\n",
    "print(\"Starting pseudonymization with command:\")\n",
    "print(cmd)\n",
    "os.system(cmd)\n",
    "\n",
    "# 5. Re-read the pseudonymized notes and update the examples.\n",
    "for nid, example in tqdm(zip(typo_noteids, examples), total=len(typo_noteids), desc=\"Processing pseudonymized notes\"):\n",
    "    left_file = os.path.join(pseudo_out_dpath, f'{nid}_left.txt')\n",
    "    right_file = os.path.join(pseudo_out_dpath, f'{nid}_right.txt')\n",
    "    if os.path.exists(left_file):\n",
    "        with open(left_file, 'r', encoding='utf-8') as fd:\n",
    "            note = fd.read()\n",
    "            example[1] = process_note(note)\n",
    "    if os.path.exists(right_file):\n",
    "        with open(right_file, 'r', encoding='utf-8') as fd:\n",
    "            note = fd.read()\n",
    "            example[2] = process_note(note)\n",
    "    # Convert the chosen target word to lowercase.\n",
    "    example[0] = example[0].lower()\n",
    "\n",
    "# 6. Generate corrupted versions (typos) of the chosen words.\n",
    "print(\"Generating corrupted words...\")\n",
    "random.seed(1234)\n",
    "correct_words = [ex[0] for ex in examples]\n",
    "typo_words = [corrupt_word(w) if w != \"\" else \"\" for w in correct_words]\n",
    "# Debug: Print a few examples of word corruption.\n",
    "for i, (cw, tw) in enumerate(zip(correct_words, typo_words)):\n",
    "    print(f\"\\t{cw} -> {tw}\")\n",
    "    if i == 5:\n",
    "        break\n",
    "print(\"Done generating corrupted words!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "052fe891-0019-48c8-a551-075158441238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation complete! Output saved to: ../data/mimic_synthetic/all.tsv\n"
     ]
    }
   ],
   "source": [
    " # 7. Generate final dataset for BERT.\n",
    "# Format (TSV): index, note_id, word, left, right, correct\n",
    "# The context is trimmed to the last 128 tokens for left and first 128 tokens for right.\n",
    "random.seed(1234)\n",
    "data_indices = list(range(num_examples))\n",
    "random.shuffle(data_indices)\n",
    "val_idx = sorted(data_indices[:num_val_examples])\n",
    "test_idx = sorted(data_indices[num_val_examples:])\n",
    "\n",
    "with open(all_output_fpath, 'w', encoding='utf-8', newline='') as fd:\n",
    "    writer = csv.writer(fd, delimiter='\\t')\n",
    "    writer.writerow(['index', 'note_id', 'word', 'left', 'right', 'correct'])\n",
    "    for i in range(num_examples):\n",
    "        nid = typo_noteids[i]\n",
    "        correct, left, right = examples[i]\n",
    "        typo = typo_words[i]\n",
    "        left_context = ' '.join(left.split()[-128:])\n",
    "        right_context = ' '.join(right.split()[:128])\n",
    "        writer.writerow([i, nid, typo, left_context, right_context, correct])\n",
    "\n",
    "print(\"Dataset generation complete! Output saved to:\", all_output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2a3cb-bf8a-43d1-b3de-e4fcd7aebe2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
