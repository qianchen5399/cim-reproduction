{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9f63de-c137-4968-9c12-40fd4cd8cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injected dummy fairseq module.\n",
      "Injected dummy fastDamerauLevenshtein module.\n",
      "Patched transformers.generation_utils with dummy top_k_top_p_filtering function.\n"
     ]
    }
   ],
   "source": [
    "# %% [code] Set Working Directory and Patch Dummy Modules / Transformers\n",
    "\n",
    "import sys, types\n",
    "\n",
    "# Dummy for fairseq\n",
    "dummy_fairseq = types.ModuleType(\"fairseq\")\n",
    "dummy_fairseq.optim = types.ModuleType(\"fairseq.optim\")\n",
    "dummy_fairseq.optim.build_optimizer = lambda args, params: None\n",
    "dummy_fairseq.optim.lr_scheduler = types.ModuleType(\"fairseq.optim.lr_scheduler\")\n",
    "dummy_fairseq.optim.lr_scheduler.build_lr_scheduler = lambda args, optimizer: None\n",
    "sys.modules[\"fairseq\"] = dummy_fairseq\n",
    "print(\"Injected dummy fairseq module.\")\n",
    "\n",
    "# Dummy for fastDamerauLevenshtein\n",
    "def damerau_levenshtein_distance(s1, s2):\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    d = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n",
    "    for i in range(len_s1 + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len_s2 + 1):\n",
    "        d[0][j] = j\n",
    "    for i in range(1, len_s1 + 1):\n",
    "        for j in range(1, len_s2 + 1):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            d[i][j] = min(\n",
    "                d[i-1][j] + 1,\n",
    "                d[i][j-1] + 1,\n",
    "                d[i-1][j-1] + cost\n",
    "            )\n",
    "            # optional transposition\n",
    "            if i > 1 and j > 1 and s1[i-1] == s2[j-2] and s1[i-2] == s2[j-1]:\n",
    "                d[i][j] = min(d[i][j], d[i-2][j-2] + cost)\n",
    "    return d[len_s1][len_s1]\n",
    "\n",
    "dummy_fast = types.ModuleType(\"fastDamerauLevenshtein\")\n",
    "dummy_fast.damerauLevenshtein = damerau_levenshtein_distance\n",
    "sys.modules[\"fastDamerauLevenshtein\"] = dummy_fast\n",
    "print(\"Injected dummy fastDamerauLevenshtein module.\")\n",
    "\n",
    "# Patch transformers.generation_utils\n",
    "import transformers\n",
    "dummy_generation_utils = types.ModuleType(\"generation_utils\")\n",
    "def dummy_top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\")):\n",
    "    return logits\n",
    "dummy_generation_utils.top_k_top_p_filtering = dummy_top_k_top_p_filtering\n",
    "setattr(transformers, \"generation_utils\", dummy_generation_utils)\n",
    "sys.modules[\"transformers.generation_utils\"] = dummy_generation_utils\n",
    "print(\"Patched transformers.generation_utils with dummy top_k_top_p_filtering function.\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", \n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c289224c-6750-407b-b273-18d4ef5f4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:52:28,829 - __main__ - INFO - Character Language Model and related classes defined successfully.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Dataset for TypoTransformer\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import torch\n",
    "import multiprocessing\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO  # Only INFO and above will be shown.\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from utils.mimic_tools import MIMICPseudonymizer\n",
    "from scripts.utils import sanitize_text, clean_text\n",
    "from fastDamerauLevenshtein import damerauLevenshtein\n",
    "\n",
    "DEFAULT_MAX_CHARACTER_POSITIONS = 64\n",
    "char_tokens = list(r\"0123456789abcdefghijklmnopqrstuvwxyz+-*/^.,;:=!?'()[]{}&\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# CharTokenizer\n",
    "###############################################################################\n",
    "class CharTokenizer(object):\n",
    "    \"\"\"\n",
    "    A simple character-level tokenizer with optional BOS/EOS/pad tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_length=DEFAULT_MAX_CHARACTER_POSITIONS):\n",
    "        self.max_length = max_length\n",
    "        self.bos, self.pad, self.eos, self.unk = ['<s>', '<pad>', '</s>', '<unk>']\n",
    "        self.bos_index, self.pad_index, self.eos_index, self.unk_index = 0, 1, 2, 3\n",
    "\n",
    "        # Build char_to_id / id_to_char for special tokens + characters.\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        special_tokens = [self.bos, self.pad, self.eos, self.unk]\n",
    "        for i, c in enumerate(special_tokens + char_tokens):\n",
    "            self.char_to_id[c] = i\n",
    "            self.id_to_char[i] = c\n",
    "\n",
    "    def tokenize(self, text, eos_bos=True, padding_end=False, max_length=None, output_token_ids=False):\n",
    "        \"\"\"\n",
    "        Tokenize text at the character level.\n",
    "        - eos_bos: whether to prepend <s> and append </s>.\n",
    "        - padding_end: whether to pad the sequence to a fixed length.\n",
    "        - max_length: override default if needed.\n",
    "        - output_token_ids: if True, return (token_ids, attention_mask);\n",
    "          otherwise return (tokens, attention_mask).\n",
    "        \"\"\"\n",
    "        assert isinstance(text, str)\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "        max_seq_len = max_length - 2 if eos_bos else max_length\n",
    "        tokens = []\n",
    "        attention_mask = []\n",
    "\n",
    "        # Convert characters up to max_seq_len\n",
    "        for char in text[:max_seq_len]:\n",
    "            tokens.append(char if char in self.char_to_id else self.unk)\n",
    "            attention_mask.append(1)\n",
    "\n",
    "\n",
    "        if eos_bos:\n",
    "            tokens.insert(0, self.bos)\n",
    "            tokens.append(self.eos)\n",
    "            attention_mask.insert(0, 1)\n",
    "            attention_mask.append(1)\n",
    "\n",
    "\n",
    "        if padding_end:\n",
    "            pad_len = max_length - len(tokens)\n",
    "            tokens.extend([self.pad] * pad_len)\n",
    "            attention_mask.extend([0] * pad_len)\n",
    "\n",
    "        if output_token_ids:\n",
    "            token_ids = self.convert_tokens_to_ids(tokens)\n",
    "            return token_ids, attention_mask\n",
    "        return tokens, attention_mask\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.char_to_id[t] for t in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.cpu().detach().tolist()\n",
    "        return [self.id_to_char[i] for i in ids]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Map-style Dataset (for validation/test)\n",
    "###############################################################################\n",
    "class TypoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset gives examples of (typo, context, correction).\n",
    "    \"\"\"\n",
    "    def __init__(self, tsv_file, bert_tokenizer, typo_tokenizer, num_process=None, max_rows=None):\n",
    "        assert os.path.exists(tsv_file), f'{tsv_file} does not exist'\n",
    "        self.tsv_file = tsv_file\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.typo_tokenizer = typo_tokenizer\n",
    "\n",
    "        print(f\"Read file {tsv_file}... \", end=\"\", flush=True)\n",
    "        self.csv_rows = []\n",
    "        with open(self.tsv_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as fd:\n",
    "            reader = csv.reader(fd, delimiter=\"\\t\")\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0:  # skip header\n",
    "                    continue\n",
    "                self.csv_rows.append(row)\n",
    "                if max_rows is not None and len(self.csv_rows) >= max_rows:\n",
    "                    break\n",
    "        print(f\"{len(self.csv_rows)} rows\")\n",
    "\n",
    "        if num_process is None:\n",
    "            num_process = multiprocessing.cpu_count()\n",
    "        num_process = min(num_process, len(self.csv_rows))\n",
    "        print(f\"Parsing rows using {num_process} process{'es' if num_process > 1 else ''}\")\n",
    "\n",
    "        self.examples = []\n",
    "        fixed_chunksize = 100\n",
    "\n",
    "        if num_process == 1:\n",
    "            for row in tqdm(self.csv_rows, total=len(self.csv_rows)):\n",
    "                self.examples.append(self._parse_row(row))\n",
    "        else:\n",
    "            pool = multiprocessing.Pool(num_process)\n",
    "            for example in tqdm(pool.imap(self._parse_row, self.csv_rows, chunksize=fixed_chunksize),\n",
    "                                total=len(self.csv_rows)):\n",
    "                self.examples.append(example)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "    def _make_sentence(self, tokens_left, tokens_right, seq_length=128):\n",
    "        len_left = len(tokens_left)\n",
    "        len_right = len(tokens_right)\n",
    "        cut_len = len_left + len_right - (seq_length - 1)\n",
    "        if cut_len > 0:\n",
    "            cut_left = len_left - seq_length // 2\n",
    "            cut_right = len_right - (seq_length - 1) // 2\n",
    "            if cut_left < 0:\n",
    "                cut_left, cut_right = 0, cut_left + cut_right\n",
    "            elif cut_right < 0:\n",
    "                cut_left, cut_right = cut_left + cut_right, 0\n",
    "        else:\n",
    "            cut_left, cut_right = 0, 0\n",
    "\n",
    "        tokens_left = tokens_left[cut_left:]\n",
    "        tokens_right = tokens_right[:len(tokens_right) - cut_right]\n",
    "        tokens = tokens_left + [self.bert_tokenizer.mask_token] + tokens_right\n",
    "        attention_mask = [1] * len(tokens_left) + [1] + [1] * len(tokens_right)\n",
    "\n",
    "        if len(tokens) < seq_length:\n",
    "            pad_len = seq_length - len(tokens)\n",
    "            tokens.extend([self.bert_tokenizer.pad_token] * pad_len)\n",
    "            attention_mask.extend([0] * pad_len)\n",
    "        return tokens, attention_mask\n",
    "\n",
    "    def _parse_row(self, row):\n",
    "        \"\"\"\n",
    "        Convert a CSV row to an example.\n",
    "        Expected format: [ex_id, note_id, typo, left, right, correct]\n",
    "        \"\"\"\n",
    "        ex_id, note_id, typo, left, right, correct = row\n",
    "        tokens_left = self.bert_tokenizer.tokenize(left)\n",
    "        tokens_right = self.bert_tokenizer.tokenize(right)\n",
    "        context_tokens, context_attention_mask = self._make_sentence(tokens_left, tokens_right)\n",
    "        context_token_ids = self.bert_tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "\n",
    "        typo_token_ids, typo_attention_mask = self.typo_tokenizer.tokenize(\n",
    "            typo, eos_bos=True, padding_end=False, output_token_ids=True)\n",
    "        correct_token_ids, correct_attention_mask = self.typo_tokenizer.tokenize(\n",
    "            correct, eos_bos=True, padding_end=False, output_token_ids=True)\n",
    "\n",
    "        return {\n",
    "            'example_id': int(ex_id),\n",
    "            'note_id': int(note_id),\n",
    "            'context_tokens': context_token_ids,\n",
    "            'context_attention_mask': context_attention_mask,\n",
    "            'typo': typo,\n",
    "            'typo_token_ids': typo_token_ids,\n",
    "            'typo_attention_mask': typo_attention_mask,\n",
    "            'correct': correct,\n",
    "            'correct_token_ids': correct_token_ids,\n",
    "            'correct_attention_mask': correct_attention_mask\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index]\n",
    "\n",
    "    def get_collate_fn(self, stack=True):\n",
    "        def _collate_fn(examples):\n",
    "            max_typo_len = max(len(e['typo_token_ids']) for e in examples)\n",
    "            max_correct_len = max(len(e['correct_token_ids']) for e in examples)\n",
    "            common_len = max(max_typo_len, max_correct_len) + 1\n",
    "            for e in examples:\n",
    "                e['typo_token_ids'] = e['typo_token_ids'] + [self.typo_tokenizer.pad_index] * (common_len - len(e['typo_token_ids']))\n",
    "                e['typo_attention_mask'] = e['typo_attention_mask'] + [0] * (common_len - len(e['typo_attention_mask']))\n",
    "                e['correct_token_ids'] = e['correct_token_ids'] + [self.typo_tokenizer.pad_index] * (common_len - len(e['correct_token_ids']))\n",
    "                e['correct_attention_mask'] = e['correct_attention_mask'] + [0] * (common_len - len(e['correct_attention_mask']))\n",
    "            if stack:\n",
    "                batch = {}\n",
    "                for key in examples[0].keys():\n",
    "                    if key in ['typo', 'correct', 'left_context', 'right_context']:\n",
    "                        batch[key] = [e[key] for e in examples]\n",
    "                    else:\n",
    "                        batch[key] = torch.tensor([e[key] for e in examples])\n",
    "                return batch\n",
    "            else:\n",
    "                return examples\n",
    "        return _collate_fn\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Iterable (Streaming) Dataset\n",
    "###############################################################################\n",
    "class TypoOnlineDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    This streaming dataset reads from multiple CSV files at random.\n",
    "    It sums the rows from all CSV files for its __len__.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_dir, dict_file, bert_tokenizer, typo_tokenizer,\n",
    "                 max_word_corruptions=3, do_substitution=True, do_transposition=True,\n",
    "                 no_corruption_prob=0.0, min_word_len=3):\n",
    "        super().__init__()\n",
    "        self.csv_dir = csv_dir\n",
    "        self.csv_fnames = [f for f in os.listdir(self.csv_dir)\n",
    "                           if f.startswith('NOTEEVENTS') and f.endswith('.csv')]\n",
    "        self.dict_file = dict_file\n",
    "        with open(self.dict_file, 'r') as fd:\n",
    "            self.dictionary = set(json.load(fd))\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.typo_tokenizer = typo_tokenizer\n",
    "\n",
    "        self.max_word_corruptions = max_word_corruptions\n",
    "        self.do_substitution = do_substitution\n",
    "        self.do_transposition = do_transposition\n",
    "        self.word_corrupter = WordCorrupter(self.max_word_corruptions,\n",
    "                                            self.do_substitution,\n",
    "                                            self.do_transposition)\n",
    "        self.no_corruption_prob = no_corruption_prob\n",
    "        self.min_word_len = min_word_len\n",
    "\n",
    "        mimic_tools_dir = 'scripts/mimic-tools/lists'\n",
    "        self.mimic_pseudo = MIMICPseudonymizer(mimic_tools_dir)\n",
    "\n",
    "        self.total_samples = self._count_total_rows()\n",
    "\n",
    "    def get_collate_fn(self, stack=True):\n",
    "        return TypoDataset.get_collate_fn(self)\n",
    "\n",
    "    def _count_total_rows(self):\n",
    "        total = 0\n",
    "        for fname in self.csv_fnames:\n",
    "            csv_path = os.path.join(self.csv_dir, fname)\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, low_memory=False)\n",
    "                total += len(df)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading {csv_path}: {e}\")\n",
    "        return total\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def _load_random_csv(self):\n",
    "        self.csv_fname = random.choice(self.csv_fnames)\n",
    "        self.csv_path = os.path.join(self.csv_dir, self.csv_fname)\n",
    "        self.df_note = pd.read_csv(self.csv_path, low_memory=False)\n",
    "        self.note_iterrows = self.df_note.iterrows()\n",
    "\n",
    "    def _make_sentence(self, tokens_left, tokens_right, seq_length=128):\n",
    "        len_left = len(tokens_left)\n",
    "        len_right = len(tokens_right)\n",
    "        cut_len = len_left + len_right - (seq_length - 1)\n",
    "        if cut_len > 0:\n",
    "            cut_left = len_left - seq_length // 2\n",
    "            cut_right = len_right - (seq_length - 1) // 2\n",
    "            if cut_left < 0:\n",
    "                cut_left, cut_right = 0, cut_left + cut_right\n",
    "            elif cut_right < 0:\n",
    "                cut_left, cut_right = cut_left + cut_right, 0\n",
    "        else:\n",
    "            cut_left, cut_right = 0, 0\n",
    "\n",
    "        tokens_left = tokens_left[cut_left:]\n",
    "        tokens_right = tokens_right[:len(tokens_right) - cut_right]\n",
    "        tokens = tokens_left + [self.bert_tokenizer.mask_token] + tokens_right\n",
    "        attention_mask = [1] * len(tokens_left) + [1] + [1] * len(tokens_right)\n",
    "        if len(tokens) < seq_length:\n",
    "            pad_len = seq_length - len(tokens)\n",
    "            tokens.extend([self.bert_tokenizer.pad_token] * pad_len)\n",
    "            attention_mask.extend([0] * pad_len)\n",
    "        return tokens, attention_mask\n",
    "\n",
    "    def _parse_row(self, row):\n",
    "        \"\"\"\n",
    "        Updated _parse_row for streaming examples.\n",
    "        Expected row format: [ex_id, note_id, typo, left, right, correct]\n",
    "        \"\"\"\n",
    "        ex_id, note_id, typo, left, right, correct = row\n",
    "        tokens_left = self.bert_tokenizer.tokenize(left)\n",
    "        tokens_right = self.bert_tokenizer.tokenize(right)\n",
    "        context_tokens, context_attention_mask = self._make_sentence(tokens_left, tokens_right)\n",
    "        context_token_ids = self.bert_tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "\n",
    "        typo_token_ids, typo_attention_mask = self.typo_tokenizer.tokenize(\n",
    "            typo, eos_bos=True, padding_end=False, output_token_ids=True)\n",
    "        correct_token_ids, correct_attention_mask = self.typo_tokenizer.tokenize(\n",
    "            correct, eos_bos=True, padding_end=False, output_token_ids=True)\n",
    "\n",
    "        return {\n",
    "            \"example_id\": ex_id,\n",
    "            \"note_id\": note_id,\n",
    "            \"context_tokens\": context_token_ids,\n",
    "            \"context_attention_mask\": context_attention_mask,\n",
    "            \"typo\": typo,\n",
    "            \"typo_token_ids\": typo_token_ids,\n",
    "            \"typo_attention_mask\": typo_attention_mask,\n",
    "            \"left_context\": left,\n",
    "            \"right_context\": right,\n",
    "            \"correct\": correct,\n",
    "            \"correct_token_ids\": correct_token_ids,\n",
    "            \"correct_attention_mask\": correct_attention_mask\n",
    "        }\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._load_random_csv()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            try:\n",
    "                _, row = next(self.note_iterrows)\n",
    "            except StopIteration:\n",
    "                self._load_random_csv()\n",
    "                _, row = next(self.note_iterrows)\n",
    "            note_id = int(row.ROW_ID)\n",
    "            note = row.TEXT.strip()\n",
    "            if len(note) < 500:\n",
    "                continue\n",
    "            try:\n",
    "                correct, left, right = self._random_word_context(note)\n",
    "            except Exception:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        correct = correct.lower()\n",
    "        typo = self.word_corrupter.corrupt_word(correct) if random.uniform(0, 1) >= self.no_corruption_prob else correct\n",
    "        left = self.mimic_pseudo.pseudonymize(left)\n",
    "        left = self._process_note(left)\n",
    "        left = ' '.join(left.split(' ')[-128:])\n",
    "        right = self.mimic_pseudo.pseudonymize(right)\n",
    "        right = self._process_note(right)\n",
    "        right = ' '.join(right.split(' ')[:128])\n",
    "        temp_csv_row = [-1, note_id, typo, left, right, correct]\n",
    "        return self._parse_row(temp_csv_row)\n",
    "\n",
    "    def _count_total_rows(self):\n",
    "        total = 0\n",
    "        for fname in self.csv_fnames:\n",
    "            csv_path = os.path.join(self.csv_dir, fname)\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, low_memory=False)\n",
    "                total += len(df)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading {csv_path}: {e}\")\n",
    "        return total\n",
    "\n",
    "    def _random_word_context(self, text, max_trial=10):\n",
    "        \"\"\"\n",
    "        Quietly tries up to max_trial times to find a suitable word in the text.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        trial = 0\n",
    "        while trial < max_trial and words:\n",
    "            trial += 1\n",
    "            word = words[0]\n",
    "            if (len(word) >= self.min_word_len and \n",
    "                word.lower() in self.dictionary and \n",
    "                len(word) < DEFAULT_MAX_CHARACTER_POSITIONS - 4):\n",
    "                left_context = \" \".join(words[:1])\n",
    "                right_context = \" \".join(words[1:])\n",
    "                return word, left_context, right_context\n",
    "            else:\n",
    "                words = words[1:]\n",
    "        return \"default\", \"\", \"\"\n",
    "\n",
    "    def _process_note(self, note):\n",
    "        note = re.sub('\\n', ' ', note)\n",
    "        note = re.sub('\\t', ' ', note)\n",
    "        return sanitize_text(clean_text(note))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Utility: WordCorrupter\n",
    "###############################################################################\n",
    "class WordCorrupter(object):\n",
    "    def __init__(self, max_corruptions=2, do_substitution=True, do_transposition=True):\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.max_corruptions = max_corruptions\n",
    "        self.do_substitution = do_substitution\n",
    "        self.do_transposition = do_transposition\n",
    "        self.operation_list = ['ins', 'del']\n",
    "        if self.do_substitution:\n",
    "            self.operation_list.append('sub')\n",
    "        if self.do_transposition:\n",
    "            self.operation_list.append('tra')\n",
    "\n",
    "    def random_alphabet(self):\n",
    "        return random.choice(self.alphabet)\n",
    "\n",
    "    def single_corruption(self, word):\n",
    "        while True:\n",
    "            oper = random.choice(self.operation_list)\n",
    "            if oper == \"del\":\n",
    "                if len(word) == 1:\n",
    "                    continue\n",
    "                cidx = random.randint(0, len(word) - 1)\n",
    "                ret = word[:cidx] + word[cidx+1:]\n",
    "                break\n",
    "            elif oper == \"ins\":\n",
    "                cidx = random.randint(0, len(word))\n",
    "                ret = word[:cidx] + self.random_alphabet() + word[cidx:]\n",
    "                break\n",
    "            elif oper == \"sub\":\n",
    "                cidx = random.randint(0, len(word) - 1)\n",
    "                while True:\n",
    "                    c = self.random_alphabet()\n",
    "                    if c != word[cidx]:\n",
    "                        ret = word[:cidx] + c + word[cidx+1:]\n",
    "                        break\n",
    "                break\n",
    "            elif oper == \"tra\":\n",
    "                if len(word) == 1:\n",
    "                    continue\n",
    "                cidx = random.randint(0, len(word) - 2)\n",
    "                if word[cidx+1] == word[cidx]:\n",
    "                    continue\n",
    "                ret = word[:cidx] + word[cidx+1] + word[cidx] + word[cidx+2:]\n",
    "                break\n",
    "            else:\n",
    "                raise ValueError(f'Wrong operation {oper}')\n",
    "        return ret\n",
    "\n",
    "    def corrupt_word(self, word_original):\n",
    "        num_corruption = random.randint(1, self.max_corruptions)\n",
    "        while True:\n",
    "            word = word_original\n",
    "            for _ in range(num_corruption):\n",
    "                word = self.single_corruption(word)\n",
    "            if word_original != word:\n",
    "                break\n",
    "        return word\n",
    "\n",
    "\n",
    "logger.info(\"Character Language Model and related classes defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd4f148-6a39-4b03-8a06-92e6100db1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.utils import GenerationMixin\n",
    "from transformers.models.bart.modeling_bart import BartDecoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import multiprocessing\n",
    "\n",
    "char_tokens = list(r\"0123456789abcdefghijklmnopqrstuvwxyz+-*/^.,;:=!?'()[]{}&\")\n",
    "special_tokens_fairseq = ['<s>', '<pad>', '</s>', '<unk>']\n",
    "\n",
    "DEFAULT_MAX_CHARACTER_POSITIONS = 64\n",
    "\n",
    "def my_prepare_bart_decoder_inputs(\n",
    "    config,\n",
    "    input_ids,\n",
    "    decoder_input_ids=None,\n",
    "    decoder_padding_mask=None,\n",
    "    causal_mask_dtype=torch.float32,\n",
    "    pad_token_id=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to do manual BART-style causal masking.\n",
    "    \"\"\"\n",
    "    if decoder_input_ids is None:\n",
    "        decoder_input_ids = input_ids\n",
    "    bsz, tgt_len = decoder_input_ids.shape\n",
    "    causal_mask = torch.full(\n",
    "        (tgt_len, tgt_len), float(\"-inf\"), dtype=causal_mask_dtype, device=decoder_input_ids.device\n",
    "    )\n",
    "    causal_mask = causal_mask.triu(1)\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(1).expand(bsz, 1, tgt_len, tgt_len)\n",
    "\n",
    "    if decoder_padding_mask is None:\n",
    "        decoder_padding_mask = decoder_input_ids.eq(pad_token_id)\n",
    "\n",
    "    return decoder_input_ids, decoder_padding_mask, causal_mask\n",
    "\n",
    "def my_make_linear_from_emb(embed_tokens):\n",
    "    \"\"\"\n",
    "    Create a linear layer whose weights tie to the given embedding matrix.\n",
    "    \"\"\"\n",
    "    weight = embed_tokens.weight\n",
    "    out_dim, in_dim = weight.shape\n",
    "    linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "    with torch.no_grad():\n",
    "        linear.weight.copy_(weight)\n",
    "    return linear\n",
    "\n",
    "class CharacterLanguageModel(transformers.BartPreTrainedModel, GenerationMixin):\n",
    "    \"\"\"\n",
    "    A custom BERT encoder + BartDecoder model for character-level correction.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, config, bert_model, char_decoder):\n",
    "        super().__init__(config)\n",
    "        self.args = args\n",
    "        self.config = config\n",
    "\n",
    "        # Final bias for vocabulary projection\n",
    "        self.final_logits_bias = nn.Parameter(torch.zeros(1, self.config.vocab_size))\n",
    "\n",
    "        # Our BERT-based encoder is inside 'bert_model.bert'\n",
    "        self.encoder = bert_model.bert\n",
    "        \n",
    "        # Our BartDecoder for the character-level decoding\n",
    "        self.decoder = char_decoder\n",
    "\n",
    "        # Optionally freeze BERT\n",
    "        self.train_bert = args.train_bert\n",
    "        if not self.train_bert:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.bos, self.pad, self.eos, self.unk = special_tokens_fairseq\n",
    "\n",
    "        # Edit-distance pools (used if you have custom ED logic, can remove if not used)\n",
    "        self.ed_pool_master = multiprocessing.pool.ThreadPool(processes=1)\n",
    "        self.ed_pool_worker = multiprocessing.Pool(processes=4)\n",
    "\n",
    "    def get_param_group(self, finetune_bert=False, finetune_factor=0.1):\n",
    "        \"\"\"\n",
    "        Return param groups with different LR factors if fine-tuning BERT is desired.\n",
    "        \"\"\"\n",
    "        if not finetune_bert:\n",
    "            return [{'params': list(self.parameters()), 'lr_factor': 1.0}]\n",
    "        else:\n",
    "            bert_params = list(self.encoder.parameters())\n",
    "            bert_params_set = set(bert_params)\n",
    "            other_params = [p for p in self.parameters() if p not in bert_params_set]\n",
    "            return [\n",
    "                {'params': other_params, 'lr_factor': 1.0},\n",
    "                {'params': bert_params, 'lr_factor': finetune_factor}\n",
    "            ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_bert_config(cls, bert_config_file):\n",
    "        with open(bert_config_file, 'r') as fd:\n",
    "            config_orig = json.load(fd)\n",
    "        bert_config = transformers.BertConfig(\n",
    "            vocab_size=config_orig['vocab_size'],\n",
    "            hidden_size=config_orig['hidden_size'],\n",
    "            num_hidden_layers=config_orig['num_hidden_layers'],\n",
    "            num_attention_heads=config_orig['num_attention_heads'],\n",
    "            intermediate_size=config_orig['intermediate_size'],\n",
    "            hidden_act=config_orig['hidden_act'],\n",
    "            hidden_dropout_prob=config_orig['hidden_dropout_prob'],\n",
    "            attention_probs_dropout_prob=config_orig['attention_probs_dropout_prob'],\n",
    "            max_position_embeddings=config_orig['max_position_embeddings'],\n",
    "            type_vocab_size=config_orig['type_vocab_size'],\n",
    "            initializer_range=config_orig['initializer_range'],\n",
    "            layer_norm_eps=1e-12\n",
    "        )\n",
    "        return bert_config\n",
    "\n",
    "    @classmethod\n",
    "    def build_bert_model(cls, bert_config):\n",
    "        \"\"\"\n",
    "        Build a BertForPreTraining model from the config\n",
    "        (We only use the 'bert' submodule for the encoder).\n",
    "        \"\"\"\n",
    "        return transformers.BertForPreTraining(bert_config)\n",
    "\n",
    "    @classmethod\n",
    "    def get_bert_tokenizer(cls, bert_vocab_file):\n",
    "        \"\"\"\n",
    "        Return a standard HF BertTokenizer from the provided vocab file.\n",
    "        \"\"\"\n",
    "        return transformers.BertTokenizer(\n",
    "            bert_vocab_file,\n",
    "            do_lower_case=True,\n",
    "            do_basic_tokenize=True\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def get_char_embeddings_from_bert(cls, bert_embeddings, bert_tokenizer):\n",
    "        \"\"\"\n",
    "        Create a new nn.Embedding for only the char tokens we care about\n",
    "        (special + '0123456...z' etc.) from the full BERT embedding matrix.\n",
    "        \"\"\"\n",
    "        char_word_ids = [\n",
    "            bert_tokenizer.cls_token_id,\n",
    "            bert_tokenizer.pad_token_id,\n",
    "            bert_tokenizer.sep_token_id,\n",
    "            bert_tokenizer.unk_token_id\n",
    "        ]\n",
    "        char_word_ids.extend(bert_tokenizer.convert_tokens_to_ids(char_tokens))\n",
    "\n",
    "        # If bert_embeddings is an nn.Embedding\n",
    "        if isinstance(bert_embeddings, nn.Embedding):\n",
    "            embedding_matrix = bert_embeddings(torch.tensor(char_word_ids)).detach()\n",
    "        # If it is a tensor\n",
    "        elif isinstance(bert_embeddings, torch.Tensor):\n",
    "            embedding_matrix = bert_embeddings[char_word_ids, :].detach()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported embedding type for get_char_embeddings_from_bert\")\n",
    "\n",
    "        return nn.Embedding.from_pretrained(\n",
    "            embedding_matrix,\n",
    "            freeze=False,      # allow fine-tuning\n",
    "            padding_idx=1      # index 1 = <pad>\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def get_char_decoder_config(cls, bert_config_file, args):\n",
    "        \"\"\"\n",
    "        Build a BartConfig for the decoder, matching the BERT hidden size, etc.\n",
    "        \"\"\"\n",
    "        with open(bert_config_file, 'r') as fd:\n",
    "            config_orig = json.load(fd)\n",
    "        return transformers.BartConfig(\n",
    "            is_decoder=True,\n",
    "            vocab_size=len(char_tokens) + len(special_tokens_fairseq),\n",
    "            d_model=config_orig['hidden_size'],\n",
    "            decoder_layers=args.decoder_layers,\n",
    "            max_position_embeddings=DEFAULT_MAX_CHARACTER_POSITIONS,\n",
    "            init_str=config_orig['initializer_range']  # can be used for init if you want\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def build_char_decoder(cls, bart_config, char_embedding):\n",
    "        \"\"\"\n",
    "        Create a BartDecoder using our char_embedding as the input embeddings.\n",
    "        \"\"\"\n",
    "        return BartDecoder(bart_config, char_embedding)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        target_correct=None,\n",
    "        encoder_outputs=None,\n",
    "        encoder_embeds=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass for the BERT encoder + BartDecoder architecture.\n",
    "        \"\"\"\n",
    "        # If encoder_embeds is passed, skip re-encoding\n",
    "        if encoder_embeds is not None:\n",
    "            encoder_outputs = (encoder_embeds,)\n",
    "\n",
    "        # 1) Encode with BERT if needed\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict\n",
    "            )\n",
    "\n",
    "        # 2) If caller didn't supply decoder inputs or masks\n",
    "        if decoder_input_ids is None and 'input_ids_correct' in kwargs:\n",
    "            decoder_input_ids = kwargs['input_ids_correct']\n",
    "        if decoder_attention_mask is None and 'attention_mask_correct' in kwargs:\n",
    "            decoder_attention_mask = kwargs['attention_mask_correct']\n",
    "\n",
    "        # 3) Decode with BartDecoder\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_outputs[0],   \n",
    "            encoder_attention_mask=attention_mask,       \n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True   \n",
    "        )\n",
    "\n",
    "        # 4) Project the decoder outputs to the vocab\n",
    "        lm_output_logits = F.linear(\n",
    "            decoder_outputs[0],               \n",
    "            self.decoder.embed_tokens.weight,  \n",
    "            bias=self.final_logits_bias\n",
    "        )\n",
    "\n",
    "        # *** Return the entire Seq2SeqLMOutput *** \n",
    "        # pass through `past_key_values` so beam search can do fast decoding\n",
    "        return transformers.modeling_outputs.Seq2SeqLMOutput(\n",
    "            logits=lm_output_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states if output_hidden_states else None,\n",
    "            decoder_attentions=decoder_outputs.attentions if output_attentions else None,\n",
    "        )\n",
    "\n",
    "    def get_encoder(self):\n",
    "        \"\"\"\n",
    "        Return the BERT encoder for convenience.\n",
    "        \"\"\"\n",
    "        return self.encoder\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        \"\"\"\n",
    "        For generation: tie the final linear layer to self.decoder.embed_tokens\n",
    "        \"\"\"\n",
    "        return my_make_linear_from_emb(self.decoder.embed_tokens)\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        decoder_input_ids,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "       return a dict of standard names that match `forward()`:\n",
    "         - input_ids / attention_mask / decoder_input_ids / (etc.)\n",
    "        By default, we won't re-encode if `encoder_outputs` is given.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": None,              \n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n",
    "        \"\"\"\n",
    "        Optionally modify the logits at each generation step\n",
    "        \"\"\"\n",
    "        if cur_len == 1 and self.config.force_bos_token_to_be_generated:\n",
    "            self._force_token_id_to_be_generated(logits, self.config.bos_token_id)\n",
    "        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:\n",
    "            self._force_token_id_to_be_generated(logits, self.config.eos_token_id)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def _force_token_id_to_be_generated(scores, token_id) -> None:\n",
    "        \"\"\"\n",
    "        Utility for forcibly setting the distribution to only the 'token_id' argument.\n",
    "        \"\"\"\n",
    "        scores[:, [x for x in range(scores.shape[1]) if x != token_id]] = float(\"-inf\")\n",
    "\n",
    "    def _reorder_cache(self, past_key_values, beam_idx):\n",
    "        \"\"\"\n",
    "        Required for beam search so it can reorder the `past_key_values` after each step.\n",
    "        We'll reorder dimension 0  according to beam_idx for each layer.\n",
    "        \"\"\"\n",
    "        if past_key_values is None:\n",
    "            return past_key_values\n",
    "\n",
    "        reordered_past = []\n",
    "        for layer_past in past_key_values:\n",
    "            if layer_past is None:\n",
    "                reordered_past.append(None)\n",
    "                continue\n",
    "\n",
    "\n",
    "            reordered_layer_past = tuple(\n",
    "                past_state.index_select(0, beam_idx) for past_state in layer_past\n",
    "            )\n",
    "            reordered_past.append(reordered_layer_past)\n",
    "\n",
    "        return tuple(reordered_past)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        HF compat: override if needed to handle weird param listing.\n",
    "        \"\"\"\n",
    "        if len(list(super().parameters())):\n",
    "            return super().parameters()\n",
    "        elif hasattr(self, \"_former_parameters\"):\n",
    "            return (p for m in self.modules() for p in m._former_parameters.values())\n",
    "        elif hasattr(self, \"_parameters\"):\n",
    "            return (p for m in self.modules() for p in m._parameters.values())\n",
    "\n",
    "    @staticmethod\n",
    "    def get_set_lr():\n",
    "        \"\"\"\n",
    "        Utility for custom LR scheduling, if code calls it.\n",
    "        \"\"\"\n",
    "        def set_lr(self, lr):\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                if 'lr_factor' in param_group:\n",
    "                    param_group['lr'] = lr * param_group['lr_factor']\n",
    "                else:\n",
    "                    param_group['lr'] = lr\n",
    "        return set_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c0fa9f-c161-47e9-9578-7e45d1f13e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic_csv_dir: data/mimic3/split\n",
      "data_dir: data/mimic_synthetic\n",
      "dict_file: data/lexicon/lexicon.json\n",
      "bert_dir: C:\\Users\\Nick Chen\\Desktop\\FinalP2\\cim-misspelling-main\\bert\\ncbi_bert_base\n",
      "output_dir: results/cim_base\n",
      "is_train: True\n",
      "test_file: test.tsv\n",
      "init_ckpt: None\n",
      "init_step: 0\n",
      "seed: 123\n",
      "decoder_layers: 12\n",
      "train_bert: True\n",
      "bert_finetune_factor: 1.0\n",
      "dropout: 0.1\n",
      "synthetic_min_word_len: 3\n",
      "do_substitution: True\n",
      "do_transposition: True\n",
      "max_word_corruptions: 3\n",
      "no_corruption_prob: 0.1\n",
      "train_with_ed: False\n",
      "atch_size: 256\n",
      "num_gpus: 1\n",
      "optimizer: adam\n",
      "adam_betas: (0.9, 0.999)\n",
      "adam_eps: 1e-08\n",
      "weight_decay: 0.01\n",
      "training_step: 60000\n",
      "display_iter: 500\n",
      "eval_iter: 5000\n",
      "lr: [0.0003]\n",
      "warmup_updates: 4000\n",
      "warmup_init_lr: 1e-06\n",
      "num_beams: 5\n",
      "edit_distance_weight: 5.0\n",
      "edit_distance_extra_len: 100\n",
      "length_penalty: 1.0\n",
      "beam_sort_linear_ed: False\n",
      "beam_final_score_normalize_ed: False\n",
      "dict_matching: True\n",
      "Re-saved TSV file to UTF-8 as: C:\\Users\\Nick Chen\\Desktop\\FinalP2\\cim-misspelling-main\\data\\mimic_synthetic\\all_fixed.tsv\n",
      "Seed, BERT configuration, and tokenizers set up successfully.\n",
      "Loading TensorFlow weights into BERT model...\n",
      "BERT weights loaded successfully.\n",
      "Creating character embeddings from BERT model...\n",
      "Building full CharacterLanguageModel...\n",
      "Full CharacterLanguageModel built successfully.\n",
      "Model initialization and checkpoint loading complete.\n",
      "Model setup complete.\n"
     ]
    }
   ],
   "source": [
    "import sys, time, pickle, random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    mimic_csv_dir=\"data/mimic3/split\",\n",
    "    data_dir=\"data/mimic_synthetic\",\n",
    "    dict_file=\"data/lexicon/lexicon.json\",\n",
    "    bert_dir=\"C:\\\\Users\\\\Nick Chen\\\\Desktop\\\\FinalP2\\\\cim-misspelling-main\\\\bert\\\\ncbi_bert_base\",\n",
    "    output_dir=\"results/cim_base\",\n",
    "    is_train=True,\n",
    "    test_file=\"test.tsv\",\n",
    "    init_ckpt=None,\n",
    "    init_step=0,\n",
    "    seed=123,\n",
    "    decoder_layers=12,\n",
    "    train_bert=True,\n",
    "    bert_finetune_factor=1.0,\n",
    "    dropout=0.1,\n",
    "    synthetic_min_word_len=3,\n",
    "    do_substitution=True,\n",
    "    do_transposition=True,\n",
    "    max_word_corruptions=3,\n",
    "    no_corruption_prob=0.1,\n",
    "    train_with_ed=False,\n",
    "    atch_size=256,\n",
    "    num_gpus=1,\n",
    "    optimizer='adam',\n",
    "    adam_betas=\"(0.9, 0.999)\",\n",
    "    adam_eps=1e-08,\n",
    "    weight_decay=0.01,\n",
    "    training_step=60000,\n",
    "    display_iter=500,\n",
    "    eval_iter=5000,\n",
    "    lr=[3e-4],\n",
    "    warmup_updates=4000,\n",
    "    warmup_init_lr=1e-6,\n",
    "    num_beams=5,\n",
    "    edit_distance_weight=5.0,\n",
    "    edit_distance_extra_len=100,\n",
    "    length_penalty=1.0,\n",
    "    beam_sort_linear_ed=False,\n",
    "    beam_final_score_normalize_ed=False,\n",
    "    dict_matching=True\n",
    ")\n",
    "\n",
    "for k, v in vars(args).items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "# Re-save TSV File with UTF-8 Encoding\n",
    "import csv\n",
    "input_path = r\"C:\\Users\\Nick Chen\\Desktop\\FinalP2\\cim-misspelling-main\\data\\mimic_synthetic\\all.tsv\"\n",
    "output_path = r\"C:\\Users\\Nick Chen\\Desktop\\FinalP2\\cim-misspelling-main\\data\\mimic_synthetic\\all_fixed.tsv\"\n",
    "with open(input_path, 'r', encoding='cp1252', errors='replace') as fin, \\\n",
    "     open(output_path, 'w', encoding='utf-8', newline='') as fout:\n",
    "    reader = csv.reader(fin, delimiter='\\t')\n",
    "    writer_csv = csv.writer(fout, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        writer_csv.writerow(row)\n",
    "print(\"Re-saved TSV file to UTF-8 as:\", output_path)\n",
    "\n",
    "from utils.checkpoint_manager import CheckPointManager\n",
    "\n",
    "bert_config_file = os.path.join(args.bert_dir, \"bert_config.json\")\n",
    "bert_config = CharacterLanguageModel.get_bert_config(bert_config_file)\n",
    "char_decoder_config = CharacterLanguageModel.get_char_decoder_config(bert_config_file, args)\n",
    "\n",
    "bert_vocab_file = os.path.join(args.bert_dir, \"vocab.txt\")\n",
    "bert_tokenizer = CharacterLanguageModel.get_bert_tokenizer(bert_vocab_file)\n",
    "\n",
    "# Build the custom CharTokenizer \n",
    "# from data.dataset import CharTokenizer  \n",
    "typo_tokenizer = CharTokenizer()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Seed, BERT configuration, and tokenizers set up successfully.\")\n",
    "\n",
    "# Build the BERT model\n",
    "bert_model = CharacterLanguageModel.build_bert_model(bert_config)\n",
    "bert_tf_file = os.path.join(args.bert_dir, \"bert_model.ckpt\")\n",
    "print(\"Loading TensorFlow weights into BERT model...\")\n",
    "bert_model.load_tf_weights(config=bert_config, tf_checkpoint_path=bert_tf_file)\n",
    "print(\"BERT weights loaded successfully.\")\n",
    "\n",
    "# Build the character decoder\n",
    "print(\"Creating character embeddings from BERT model...\")\n",
    "char_embeddings_decoder = CharacterLanguageModel.get_char_embeddings_from_bert(\n",
    "    bert_model.cls.predictions.decoder.weight, \n",
    "    bert_tokenizer\n",
    ")\n",
    "char_decoder = CharacterLanguageModel.build_char_decoder(char_decoder_config, char_embeddings_decoder)\n",
    "\n",
    "# Build the full CharacterLanguageModel\n",
    "print(\"Building full CharacterLanguageModel...\")\n",
    "model = CharacterLanguageModel(args, char_decoder_config, bert_model, char_decoder)\n",
    "# Make sure your model is recognized as an encoder-decoder:\n",
    "model.config.is_encoder_decoder = True\n",
    "\n",
    "\n",
    "model.config.bos_token_id = 0\n",
    "model.config.pad_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "model.config.unk_token_id = 3\n",
    "\n",
    "\n",
    "model.config.decoder_start_token_id = 0\n",
    "\n",
    "# Make sure config matches the total number of char tokens\n",
    "model.config.vocab_size = len(typo_tokenizer.char_to_id)\n",
    "\n",
    "print(\"Full CharacterLanguageModel built successfully.\")\n",
    "\n",
    "ckpt_manager = CheckPointManager(args.output_dir)\n",
    "init_step = args.init_step\n",
    "if args.init_ckpt is not None:\n",
    "    print(f\"Loading model checkpoint from {args.init_ckpt} (step {args.init_step})\")\n",
    "    model.load_state_dict(torch.load(args.init_ckpt))\n",
    "    init_step = args.init_step\n",
    "else:\n",
    "    ckpt_status = ckpt_manager.get_latest_checkpoint()\n",
    "    if ckpt_status:\n",
    "        init_step, ckpt_fname = ckpt_status\n",
    "        ckpt_fpath = os.path.join(args.output_dir, ckpt_fname)\n",
    "        print(f\"Loading model checkpoint from {ckpt_fpath} (step {init_step})\")\n",
    "        ckpt_manager.load_ckpt(model, ckpt_fname)\n",
    "\n",
    "global_step = init_step\n",
    "if global_step == 0:\n",
    "    ckpt_manager.save_args(args)\n",
    "print(\"Model initialization and checkpoint loading complete.\")\n",
    "\n",
    "available_gpus = torch.cuda.device_count()\n",
    "args.num_gpus = min(args.num_gpus, available_gpus)\n",
    "\n",
    "if args.num_gpus > 1:\n",
    "    model_train = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpus)))\n",
    "else:\n",
    "    model_train = model\n",
    "\n",
    "print(\"Model setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5e1d2f-4f39-4981-9b35-c6c3c01c4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset size: 416636\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the training dataset \n",
    "dataset_train = TypoOnlineDataset(\n",
    "    csv_dir=args.mimic_csv_dir,\n",
    "    dict_file=args.dict_file,\n",
    "    bert_tokenizer=bert_tokenizer,\n",
    "    typo_tokenizer=typo_tokenizer,\n",
    "    max_word_corruptions=args.max_word_corruptions,\n",
    "    do_substitution=args.do_substitution,\n",
    "    do_transposition=args.do_transposition,\n",
    "    no_corruption_prob=args.no_corruption_prob,\n",
    "    min_word_len=args.synthetic_min_word_len\n",
    ")\n",
    "\n",
    "# Calculate subset size as 10% of the training dataset's length\n",
    "subset_size = int(0.1 * len(dataset_train))\n",
    "print(f\"Using subset size: {subset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275d0a2d-4063-4342-856e-dac36fdd4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cim_decode.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from functools import lru_cache\n",
    "from fastDamerauLevenshtein import damerauLevenshtein as _ed\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# tiny memoizer  avoid recomputing ED for identical partial strings\n",
    "@lru_cache(maxsize=10_000)\n",
    "def _ed_cached(a: str, b: str) -> int:\n",
    "    return _ed(a, b)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def generate_with_edit_distance(\n",
    "    model,\n",
    "    context_tokens,               # (1, L)  torch.LongTensor\n",
    "    context_attention_mask,       # (1, L)  torch.LongTensor / BoolTensor\n",
    "    typo_str: str,                # observed misspelling, e.g. \"lugns\"\n",
    "    max_length: int = 64,\n",
    "    beam_size: int = 5,\n",
    "    C: float = 5.0,               # editdistance weight\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Onetokenatatime beam search with rescoring:\n",
    "        score = logP(y|x)CED(y,typo_str).\n",
    "    Returns the best candidate string (no BOS/EOS/pad symbols).\n",
    "    \"\"\"\n",
    "\n",
    "    bos_id, pad_id, eos_id = (model.config.bos_token_id,\n",
    "                              model.config.pad_token_id,\n",
    "                              model.config.eos_token_id)\n",
    "    id2char = model.args.typo_tokenizer.id_to_char\n",
    "\n",
    "    #  1) encode context once \n",
    "    with torch.no_grad():\n",
    "        enc_out = model.encoder(input_ids=context_tokens.to(device),\n",
    "                                attention_mask=context_attention_mask.to(device),\n",
    "                                return_dict=True)\n",
    "    mem, mem_mask = enc_out.last_hidden_state, context_attention_mask.to(device)\n",
    "\n",
    "    #  2) beam containers  (ids, score, past_key_values) \n",
    "    beams = [([bos_id], 0.0, None)]\n",
    "\n",
    "    for step in range(max_length):\n",
    "\n",
    "        all_next = []\n",
    "        finished = True\n",
    "\n",
    "        for ids, score, past in beams:\n",
    "\n",
    "            # keep finished sequences untouched\n",
    "            if ids[-1] == eos_id:\n",
    "                all_next.append((ids, score, past))\n",
    "                continue\n",
    "\n",
    "            finished = False\n",
    "\n",
    "            #  incremental decode one token \n",
    "            dec_in = torch.tensor([[ids[-1]]], device=device)\n",
    "\n",
    "            out = model.decoder(input_ids=dec_in,\n",
    "                                encoder_hidden_states=mem,\n",
    "                                encoder_attention_mask=mem_mask,\n",
    "                                past_key_values=past,\n",
    "                                use_cache=True,\n",
    "                                return_dict=True)\n",
    "\n",
    "            logits = F.linear(out.last_hidden_state[:, -1, :],\n",
    "                              model.decoder.embed_tokens.weight,\n",
    "                              bias=model.final_logits_bias)\n",
    "\n",
    "            logp, topk = torch.topk(F.log_softmax(logits, -1),\n",
    "                                    beam_size * 2, dim=-1)  # grab a few extras\n",
    "            logp, topk = logp.squeeze(0), topk.squeeze(0)\n",
    "\n",
    "            for lp, tok in zip(logp.tolist(), topk.tolist()):\n",
    "                new_ids   = ids + [tok]\n",
    "                chars     = [id2char[t] for t in new_ids if t not in (bos_id, pad_id, eos_id)]\n",
    "                partial   = \"\".join(chars)\n",
    "                new_score = score + lp - C * _ed_cached(partial, typo_str)\n",
    "                all_next.append((new_ids, new_score, out.past_key_values))\n",
    "\n",
    "        #  3) prune to best k beams \n",
    "        all_next.sort(key=lambda t: t[1], reverse=True)\n",
    "        beams = all_next[:beam_size]\n",
    "\n",
    "        if finished:           \n",
    "            break\n",
    "\n",
    "    best_ids, *_ = beams[0]\n",
    "\n",
    "    # strip BOS/EOS\n",
    "    if best_ids and best_ids[0] == bos_id:\n",
    "        best_ids = best_ids[1:]\n",
    "    if best_ids and best_ids[-1] == eos_id:\n",
    "        best_ids = best_ids[:-1]\n",
    "\n",
    "    return \"\".join(id2char[t] for t in best_ids if t not in (bos_id, pad_id, eos_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86a534fc-9f33-433e-b494-9e4b60a49c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset size: 416636\n",
      "Optimizer and LR scheduler setup complete.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#################################################################\n",
    "# 1) Create a SubsampleIterableDataset for debugging\n",
    "#################################################################\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class SubsampleIterableDataset(IterableDataset):\n",
    "    def __init__(self, dataset, max_samples):\n",
    "        self.dataset = dataset\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        for sample in self.dataset:\n",
    "            if count >= self.max_samples:\n",
    "                break\n",
    "            yield sample\n",
    "            count += 1\n",
    "\n",
    "    def get_collate_fn(self, stack=True):\n",
    "        return self.dataset.get_collate_fn(stack=stack)\n",
    "\n",
    "# create a smaller subset \n",
    "subset_size = int(0.1 * len(dataset_train))\n",
    "print(f\"Using subset size: {subset_size}\")\n",
    "subsampled_dataset = SubsampleIterableDataset(dataset_train, subset_size)\n",
    "\n",
    "\n",
    "\n",
    "# quickanddirty loader for the 10% subset\n",
    "debug_batch_size  = 16          \n",
    "debug_num_workers = 0          \n",
    "\n",
    "dataloader_train_debug = DataLoader(\n",
    "    subsampled_dataset,\n",
    "    batch_size   = debug_batch_size,\n",
    "    num_workers  = debug_num_workers,\n",
    "    collate_fn   = subsampled_dataset.get_collate_fn(),\n",
    "    pin_memory   = torch.cuda.is_available()   \n",
    ")\n",
    "\n",
    "#################################################################\n",
    "# 2) Setup an optimizer and LR scheduler for training\n",
    "#################################################################\n",
    "if not args.train_bert:\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "else:\n",
    "    # Fine-tune BERT as well\n",
    "    params = model.get_param_group(finetune_bert=True, finetune_factor=args.bert_finetune_factor)\n",
    "\n",
    "optimizer = optim.Adam(params, lr=args.lr[0], eps=args.adam_eps, weight_decay=args.weight_decay)\n",
    "\n",
    "lr_lambda = lambda step: min((step + 1) ** (-0.5), (step + 1) * args.warmup_init_lr)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "print(\"Optimizer and LR scheduler setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab6ea8c-917f-4337-ad2d-c409af883b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Crossentropy that ignores padding (uses .reshape instead of .view)\n",
    "##############################################################################\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, targets, target_mask=None):\n",
    "        bsz, seq_len, vocab_size = logits.shape\n",
    "        logits_2d  = logits.reshape(bsz * seq_len, vocab_size)  \n",
    "        targets_1d = targets.reshape(bsz * seq_len)             \n",
    "\n",
    "        log_probs = F.log_softmax(logits_2d, dim=-1)\n",
    "        nll_loss  = -log_probs[range(bsz * seq_len), targets_1d]\n",
    "\n",
    "        if target_mask is not None:\n",
    "            mask_1d = target_mask.reshape(bsz * seq_len).float() \n",
    "            loss = (nll_loss * mask_1d).sum() / mask_1d.sum()\n",
    "        else:\n",
    "            loss = nll_loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# Tokenlevel accuracy & F1  \n",
    "##############################################################################\n",
    "def compute_accuracy_f1(logits, labels, label_mask):\n",
    "    preds       = logits.argmax(dim=-1)\n",
    "\n",
    "    preds_flat  = preds.reshape(-1).cpu().numpy()    #  changed\n",
    "    labels_flat = labels.reshape(-1).cpu().numpy()   #  changed\n",
    "    mask_flat   = label_mask.reshape(-1).cpu().numpy()  #  changed\n",
    "\n",
    "    valid       = (mask_flat == 1)\n",
    "    preds_valid = preds_flat[valid]\n",
    "    labels_valid= labels_flat[valid]\n",
    "\n",
    "    correct = (preds_valid == labels_valid).sum()\n",
    "    total   = len(preds_valid)\n",
    "    acc     = correct / total if total > 0 else 0.0\n",
    "\n",
    "    positive_label_id = 42\n",
    "    pred_pos = (preds_valid  == positive_label_id)\n",
    "    labl_pos = (labels_valid == positive_label_id)\n",
    "\n",
    "    tp = (pred_pos & labl_pos).sum()\n",
    "    fp = (pred_pos & ~labl_pos).sum()\n",
    "    fn = (~pred_pos & labl_pos).sum()\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall    = tp / (tp + fn + 1e-8)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ca85e1-ece4-498c-801b-475013f10858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "def latest_full_ckpt(self, ext=\".pt\"):\n",
    "    \"\"\"\n",
    "    Return full path of the newest ckpt-<step>.<ext> file in self.output_dir,\n",
    "    or None if none exist.\n",
    "    \"\"\"\n",
    "    pat = re.compile(r\"ckpt-(\\d+)\" + re.escape(ext) + r\"$\")\n",
    "    ckpts = []\n",
    "    for f in os.listdir(self.output_dir):\n",
    "        m = pat.match(f)\n",
    "        if m:\n",
    "            ckpts.append((int(m.group(1)), f))\n",
    "    if not ckpts:\n",
    "        return None\n",
    "    ckpts.sort(key=lambda t: t[0])           # by step\n",
    "    return os.path.join(self.output_dir, ckpts[-1][1])\n",
    "\n",
    "# monkeypatch the method\n",
    "CheckPointManager.latest_full_ckpt = latest_full_ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f85350aa-e823-4719-af7b-458baced9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_manager = CheckPointManager(args.output_dir)   # or whatever you pass\n",
    "ckpt_manager.output_dir = ckpt_manager.checkpoint_dir   # < alias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884b1466-7277-4750-8717-474a410d08d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG]   Resuming from results/cim_base\\ckpt-45000.pkl\n",
      "[LOG]    resumed @ step 45,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  76%|  | 45500/60000 [21:07<10:15:18,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  45500 | loss 0.2149 | lr 4.69e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  77%|  | 46000/60000 [42:32<9:54:53,  2.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  46000 | loss 0.2050 | lr 4.66e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  78%|  | 46500/60000 [1:03:39<9:31:14,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  46500 | loss 0.2096 | lr 4.64e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  78%|  | 47000/60000 [1:24:43<9:13:02,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  47000 | loss 0.1788 | lr 4.61e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  79%|  | 47500/60000 [1:45:48<8:40:37,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  47500 | loss 0.2073 | lr 4.59e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  80%|  | 48000/60000 [2:06:53<8:12:58,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  48000 | loss 0.1897 | lr 4.56e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  81%|  | 48500/60000 [2:27:43<7:47:55,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  48500 | loss 0.2137 | lr 4.54e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  82%| | 49000/60000 [2:48:30<7:32:04,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  49000 | loss 0.1935 | lr 4.52e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  82%| | 49500/60000 [3:09:15<7:22:16,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  49500 | loss 0.1961 | lr 4.49e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  83%| | 50000/60000 [3:30:01<6:56:49,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  50000 | loss 0.2077 | lr 4.47e-07\n",
      "[LOG]  checkpoint saved  results/cim_base\\ckpt-50000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  84%| | 50500/60000 [3:52:00<6:49:31,  2.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  50500 | loss 0.2298 | lr 4.45e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  85%| | 51000/60000 [4:13:16<6:25:39,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  51000 | loss 0.1901 | lr 4.43e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  86%| | 51500/60000 [4:33:50<5:45:06,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  51500 | loss 0.2141 | lr 4.41e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  87%| | 52000/60000 [4:54:02<5:26:40,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  52000 | loss 0.2055 | lr 4.39e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  88%| | 52500/60000 [5:14:10<4:57:31,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  52500 | loss 0.2012 | lr 4.36e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  88%| | 53000/60000 [5:34:17<4:38:19,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  53000 | loss 0.2009 | lr 4.34e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  89%| | 53500/60000 [5:54:24<4:19:40,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  53500 | loss 0.2041 | lr 4.32e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  90%| | 54000/60000 [6:14:31<4:02:07,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  54000 | loss 0.2142 | lr 4.30e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  91%| | 54500/60000 [6:34:40<3:42:55,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  54500 | loss 0.2262 | lr 4.28e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  92%|| 55000/60000 [6:54:44<3:19:49,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  55000 | loss 0.1978 | lr 4.26e-07\n",
      "[LOG]  checkpoint saved  results/cim_base\\ckpt-55000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  92%|| 55500/60000 [7:15:32<3:09:06,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  55500 | loss 0.2180 | lr 4.24e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  93%|| 56000/60000 [7:36:21<2:47:13,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  56000 | loss 0.2040 | lr 4.23e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  94%|| 56500/60000 [7:57:08<2:26:15,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  56500 | loss 0.2012 | lr 4.21e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  95%|| 57000/60000 [8:17:56<2:06:54,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  57000 | loss 0.2087 | lr 4.19e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  96%|| 57500/60000 [8:38:45<1:44:05,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  57500 | loss 0.2558 | lr 4.17e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  97%|| 58000/60000 [8:59:24<1:22:28,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  58000 | loss 1.1318 | lr 4.15e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  98%|| 58500/60000 [9:19:58<1:00:59,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  58500 | loss 0.9315 | lr 4.13e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  98%|| 59000/60000 [9:40:30<41:36,  2.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  59000 | loss 0.8520 | lr 4.12e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  99%|| 59500/60000 [10:01:01<20:04,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  59500 | loss 0.8719 | lr 4.10e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 60000/60000 [10:21:33<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] step  60000 | loss 0.8315 | lr 4.08e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 60000/60000 [10:21:37<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG]  checkpoint saved  results/cim_base\\ckpt-60000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG]  checkpoint saved  results/cim_base\\ckpt-60000.pkl\n",
      "[LOG] training finished  final step 60,000\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, re, torch, math, json, shutil\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# \n",
    "# 0)  Hyperparams / flags\n",
    "# \n",
    "debug_batch_size  = 8\n",
    "debug_num_workers = 0\n",
    "use_amp           = True\n",
    "\n",
    "args.training_step = 60000\n",
    "display_every      = args.display_iter\n",
    "\n",
    "# \n",
    "# 1)  DataLoader on 10% subset\n",
    "# \n",
    "subsampled_dataset = SubsampleIterableDataset(dataset_train, subset_size)\n",
    "train_loader = DataLoader(\n",
    "    subsampled_dataset,\n",
    "    batch_size  = debug_batch_size,\n",
    "    num_workers = debug_num_workers,\n",
    "    collate_fn  = subsampled_dataset.get_collate_fn(),\n",
    "    pin_memory  = torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# \n",
    "# 2)  Optimiser, scheduler, AMP\n",
    "# \n",
    "if args.train_bert:\n",
    "    params = model.get_param_group(finetune_bert=True, finetune_factor=args.bert_finetune_factor)\n",
    "else:\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(params,\n",
    "                       lr=args.lr[0],\n",
    "                       eps=args.adam_eps,\n",
    "                       weight_decay=args.weight_decay)\n",
    "\n",
    "lr_lambda    = lambda s: min((s+1)**(-0.5), (s+1)*args.warmup_init_lr)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "scaler       = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "# \n",
    "# 3)  Add / patch checkpoint helpers (pkl only)\n",
    "# \n",
    "def latest_full_ckpt(self, ext=\".pkl\"):\n",
    "    base = getattr(self, \"output_dir\", None) or getattr(self, \"checkpoint_dir\", None)\n",
    "    if base is None:\n",
    "        raise AttributeError(\"CheckpointManager lacks dir attribute\")\n",
    "    pat = re.compile(r\"ckpt-(\\d+)\" + re.escape(ext) + r\"$\")\n",
    "    files = [(int(m.group(1)), f) for f in os.listdir(base) if (m := pat.match(f))]\n",
    "    return None if not files else os.path.join(base, sorted(files)[-1][1])\n",
    "\n",
    "CheckPointManager.latest_full_ckpt = latest_full_ckpt\n",
    "ckpt_manager.output_dir = ckpt_manager.checkpoint_dir\n",
    "\n",
    "MAX_TO_KEEP = 5\n",
    "def save_full_ckpt(step):\n",
    "    path = os.path.join(ckpt_manager.output_dir, f\"ckpt-{step}.pkl\")  \n",
    "    os.makedirs(ckpt_manager.output_dir, exist_ok=True)\n",
    "    torch.save({\n",
    "        \"step\"     : step,\n",
    "        \"model\"    : model_train.module.state_dict() if args.num_gpus > 1 else model_train.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": lr_scheduler.state_dict(),\n",
    "        \"scaler\"   : scaler.state_dict(),\n",
    "    }, path)\n",
    "    ckpts = sorted([f for f in os.listdir(ckpt_manager.output_dir) if f.startswith(\"ckpt-\") and f.endswith(\".pkl\")])\n",
    "    while len(ckpts) > MAX_TO_KEEP:\n",
    "        os.remove(os.path.join(ckpt_manager.output_dir, ckpts.pop(0)))\n",
    "    print(f\"[LOG]  checkpoint saved  {path}\")\n",
    "\n",
    "# \n",
    "# 4)  Resume\n",
    "# \n",
    "resume_path = ckpt_manager.latest_full_ckpt()\n",
    "global_step = 0\n",
    "if resume_path and os.path.isfile(resume_path):\n",
    "    print(f\"[LOG]   Resuming from {resume_path}\")\n",
    "    state = torch.load(resume_path, map_location=\"cpu\")\n",
    "    model_train.load_state_dict(state[\"model\"])\n",
    "    optimizer.load_state_dict(state[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(state[\"scheduler\"])\n",
    "    scaler.load_state_dict(state.get(\"scaler\", scaler.state_dict()))\n",
    "    global_step = state[\"step\"]\n",
    "    print(f\"[LOG]    resumed @ step {global_step:,}\")\n",
    "else:\n",
    "    print(\"[LOG]  fresh run\")\n",
    "\n",
    "# \n",
    "# 5)  Train\n",
    "# \n",
    "criterion    = CrossEntropyLoss()\n",
    "pbar         = tqdm(total=args.training_step, initial=global_step, desc=\"train\")\n",
    "running_loss = 0.0\n",
    "train_iter   = iter(train_loader)\n",
    "model_train.train()\n",
    "\n",
    "while global_step < args.training_step:\n",
    "    try:\n",
    "        batch = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        batch = next(train_iter)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    ctx_ids  = batch[\"context_tokens\"].to(model_train.device, non_blocking=True)\n",
    "    ctx_mask = batch[\"context_attention_mask\"].to(model_train.device, non_blocking=True)\n",
    "    tgt_ids  = batch[\"correct_token_ids\"].to(model_train.device, non_blocking=True)\n",
    "    tgt_mask = batch[\"correct_attention_mask\"].to(model_train.device, non_blocking=True)\n",
    "\n",
    "    dec_in   = tgt_ids[:, :-1]\n",
    "    labels   = tgt_ids[:, 1:]\n",
    "    lbl_mask = tgt_mask[:, 1:].float()\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        out  = model_train(\n",
    "            input_ids              = ctx_ids,\n",
    "            attention_mask         = ctx_mask,\n",
    "            decoder_input_ids      = dec_in,\n",
    "            decoder_attention_mask = lbl_mask,\n",
    "            target_correct         = labels,\n",
    "        )\n",
    "        loss = criterion(out.logits, labels, target_mask=lbl_mask)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    global_step  += 1\n",
    "    running_loss += loss.item()\n",
    "    pbar.update(1)\n",
    "\n",
    "    if global_step % display_every == 0:\n",
    "        avg_loss = running_loss / display_every\n",
    "        running_loss = 0.0\n",
    "        lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"[LOG] step {global_step:>6} | loss {avg_loss:.4f} | lr {lr_now:.2e}\")\n",
    "\n",
    "    if global_step % args.eval_iter == 0:\n",
    "        save_full_ckpt(global_step)\n",
    "\n",
    "pbar.close()\n",
    "save_full_ckpt(global_step)\n",
    "print(f\"[LOG] training finished  final step {global_step:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a1888-1ff8-47d9-ab11-161875151bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f568ef3-6578-4ec9-918a-04e9660a6629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05612304-fed0-469d-9f78-4a95a23e9b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723bd68-6638-4dc9-86e7-373e67f42ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130fdec-b83d-4374-8e2a-d00ff9179bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ccae83-f17e-4481-97a2-d5b9e1b4f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  checkpoint loaded: results/cim_base/ckpt-15.pkl (step 15)\n",
      " evaluating \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258beda53f4d4085842813e47aa5235d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========  RESULTS  ===========\n",
      "tokens evaluated      : 12,456\n",
      "average CEloss       : 4.3426\n",
      "token accuracy        : 1.73 %\n",
      "----------------------------------\n",
      "F1 for token '</s>' : 0.0000 (P=0.0000, R=0.0000)\n",
      "MicroF1 (all classes): 0.0173 (P=0.0173, R=0.0173)\n",
      "MacroF1 (all classes): 0.0129\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LOSS that IGNORES <pad> \n",
    "class CELossIgnorePad(nn.Module):\n",
    "    def __init__(self, pad_id:int):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "    def forward(self, logits, targets):\n",
    "        V = logits.size(-1)\n",
    "        return F.cross_entropy(\n",
    "            logits.reshape(-1, V), targets.reshape(-1),\n",
    "            ignore_index=self.pad_id, reduction=\"sum\")\n",
    "\n",
    "loss_fn = CELossIgnorePad(pad_id=typo_tok.pad_index)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  ADDON METRIC STORAGE\n",
    "# ------------------------------------------------------------------\n",
    "POS_ID          = typo_tok.eos_index        # treat </s> as positive class\n",
    "tp = fp = fn    = 0                         # for singleclass F1\n",
    "all_preds, all_labels = [], []              # for micro / macro F1\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#  EVALUATION LOOP\n",
    "loss_sum  = 0.0\n",
    "tok_corr  = 0\n",
    "tok_total = 0\n",
    "\n",
    "print(\" evaluating \")\n",
    "with torch.inference_mode():                    # faster than no_grad() on PT 2.x\n",
    "    for batch in tqdm(test_loader, desc=\"eval\"):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        ctx_ids   = batch[\"context_tokens\"].to(device)\n",
    "        ctx_mask  = batch[\"context_attention_mask\"].to(device)\n",
    "        tgt_ids   = batch[\"correct_token_ids\"].to(device)\n",
    "        tgt_mask  = batch[\"correct_attention_mask\"].to(device).bool()\n",
    "\n",
    "        # teacherforcing shift\n",
    "        dec_in  = tgt_ids[:, :-1]\n",
    "        labels  = tgt_ids[:, 1:]\n",
    "        lbl_msk = tgt_mask[:, 1:]\n",
    "\n",
    "        logits = model(input_ids              = ctx_ids,\n",
    "                       attention_mask         = ctx_mask,\n",
    "                       decoder_input_ids      = dec_in,\n",
    "                       decoder_attention_mask = lbl_msk,\n",
    "                       target_correct         = labels).logits\n",
    "\n",
    "        # ------- loss & accuracy -------\n",
    "        loss_sum  += loss_fn(logits, labels).item()\n",
    "\n",
    "        preds     = logits.argmax(-1)\n",
    "        tok_corr += (preds.eq(labels) & lbl_msk).sum().item()\n",
    "        tok_total+= lbl_msk.sum().item()\n",
    "\n",
    "        # ------- singleclass (POS_ID) counters -------\n",
    "        tp += ((preds == POS_ID) & (labels == POS_ID) & lbl_msk).sum().item()\n",
    "        fp += ((preds == POS_ID) & (labels != POS_ID) & lbl_msk).sum().item()\n",
    "        fn += ((preds != POS_ID) & (labels == POS_ID) & lbl_msk).sum().item()\n",
    "\n",
    "        # ------- collect for global micro/macro F1 -------\n",
    "        all_preds .extend(preds [lbl_msk].cpu().tolist())\n",
    "        all_labels.extend(labels[lbl_msk].cpu().tolist())\n",
    "\n",
    "\n",
    "#   REPORT\n",
    "avg_loss = loss_sum / tok_total\n",
    "acc      = tok_corr / tok_total\n",
    "\n",
    "# singleclass precision / recall / F1\n",
    "prec_pos = tp / (tp + fp + 1e-8)\n",
    "rec_pos  = tp / (tp + fn + 1e-8)\n",
    "f1_pos   = 2 * prec_pos * rec_pos / (prec_pos + rec_pos + 1e-8)\n",
    "\n",
    "# micro / macro F1 with scikitlearn  (install it once:  pip install scikit-learn)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "p_micro, r_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='micro', zero_division=0)\n",
    "p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n===========  RESULTS  ===========\")\n",
    "print(f\"tokens evaluated      : {tok_total:,}\")\n",
    "print(f\"average CEloss       : {avg_loss:.4f}\")\n",
    "print(f\"token accuracy        : {acc*100:.2f} %\")\n",
    "print(\"----------------------------------\")\n",
    "print(f\"F1 for token {typo_tok.id_to_char[POS_ID]!r} : {f1_pos:.4f} \"\n",
    "      f\"(P={prec_pos:.4f}, R={rec_pos:.4f})\")\n",
    "print(f\"MicroF1 (all classes): {f1_micro:.4f} (P={p_micro:.4f}, R={r_micro:.4f})\")\n",
    "print(f\"MacroF1 (all classes): {f1_macro:.4f}\")\n",
    "print(\"=================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae2114e-449c-4566-84e7-7a79e6153771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wrote 2000 rows to data\\mimic_synthetic\\test2k.tsv\n"
     ]
    }
   ],
   "source": [
    "import csv, random, pathlib, shutil\n",
    "\n",
    "#   CONFIG  \n",
    "SRC_FILE = pathlib.Path(\n",
    "    r\"data/mimic_synthetic/all_fixed.tsv\"   # your full file (UTF8)\n",
    ")\n",
    "N_TEST   = 2_000                            # <-- change here\n",
    "SEED     = 2025                             # keep fixed for reproducibility\n",
    "DST_FILE = SRC_FILE.with_name(f\"test{N_TEST//1000}k.tsv\")\n",
    "# e.g. all_fixed.tsv -> test2k.tsv\n",
    "\n",
    "#   SPLIT  \n",
    "random.seed(SEED)\n",
    "\n",
    "# First count lines (skip header)\n",
    "num_rows = sum(1 for _ in SRC_FILE.open()) - 1\n",
    "if N_TEST > num_rows:\n",
    "    raise ValueError(f\"File only has {num_rows} data rows but N_TEST={N_TEST}\")\n",
    "\n",
    "test_idx = set(random.sample(range(num_rows), N_TEST))\n",
    "\n",
    "with (SRC_FILE.open(\"r\", encoding=\"utf8\") as fin,\n",
    "      DST_FILE.open(\"w\", encoding=\"utf8\", newline=\"\") as ftest):\n",
    "    \n",
    "    reader = csv.reader(fin,  delimiter=\"\\t\")\n",
    "    writer = csv.writer(ftest, delimiter=\"\\t\")\n",
    "\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for i, row in enumerate(reader):\n",
    "        if i in test_idx:\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(f\"  wrote {N_TEST} rows to {DST_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43221fe9-2cec-472a-aef3-a7baed8456c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c9dba-331b-45c6-a71c-9e52ff9cf1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2896e9-1501-4455-be5d-25007677b78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "844067e6-b85b-40e4-9dbb-e976eb64e0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Patched fastDamerauLevenshtein.damerauLevenshtein\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# QUICK PATCH for the dummy fastDamerauLevenshtein implementation\n",
    "# ---------------------------------------------------------------------------\n",
    "import sys, types\n",
    "\n",
    "def _patched_damerau_levenshtein(s1: str, s2: str) -> int:\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    d = [[0]*(len_s2+1) for _ in range(len_s1+1)]\n",
    "\n",
    "    for i in range(len_s1+1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len_s2+1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1+1):\n",
    "        for j in range(1, len_s2+1):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            d[i][j] = min(\n",
    "                d[i-1][j]   + 1,          # deletion\n",
    "                d[i][j-1]   + 1,          # insertion\n",
    "                d[i-1][j-1] + cost        # substitution\n",
    "            )\n",
    "            # optional transposition\n",
    "            if i > 1 and j > 1 and s1[i-1] == s2[j-2] and s1[i-2] == s2[j-1]:\n",
    "                d[i][j] = min(d[i][j], d[i-2][j-2] + cost)\n",
    "\n",
    "    return d[len_s1][len_s2]          #  correct column index!\n",
    "\n",
    "# overwrite the dummy module that was injected earlier\n",
    "dummy_fast = sys.modules.get(\"fastDamerauLevenshtein\")\n",
    "if dummy_fast is None:\n",
    "    dummy_fast = types.ModuleType(\"fastDamerauLevenshtein\")\n",
    "    sys.modules[\"fastDamerauLevenshtein\"] = dummy_fast\n",
    "dummy_fast.damerauLevenshtein = _patched_damerau_levenshtein\n",
    "print(\" Patched fastDamerauLevenshtein.damerauLevenshtein\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0270f3a7-60a4-4e2b-b6e6-d7b5b56163ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastDamerauLevenshtein import damerauLevenshtein as _ed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b665738-f696-4afe-ad85-ab085c5def3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " generate_with_edit_distance now uses the patched ED routine\n"
     ]
    }
   ],
   "source": [
    "#  grab the patched ED function from the dummy module we created earlier\n",
    "from fastDamerauLevenshtein import damerauLevenshtein as patched_ed\n",
    "\n",
    "#  point the global symbol used inside generate_with_edit_distance to it\n",
    "generate_with_edit_distance.__globals__['_ed'] = patched_ed\n",
    "\n",
    "#  clear the old cache so no corrupted results remain\n",
    "generate_with_edit_distance.__globals__['_ed_cached'].cache_clear()\n",
    "\n",
    "print(\" generate_with_edit_distance now uses the patched ED routine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85b6b708-a796-43d9-b8b3-4e4fb5bc21e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " device: cuda\n",
      " loading test set \n",
      "2000 rows data/mimic_synthetic/test2k.tsv... \n",
      "Parsing rows using 1 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2000/2000 [00:06<00:00, 324.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2,000 examples\n",
      "  checkpoint loaded: results/cim_base/ckpt-20000.pkl (step 20000)\n",
      " evaluating \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval TFloss: 100%|| 63/63 [08:34<00:00,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================  RESULTS  =================\n",
      "tokens evaluated (TF)  : 12,456\n",
      "average CEloss        : 3.0717\n",
      "token accuracy (TF)    : 21.23 %\n",
      "--------------------------------------------\n",
      "wordlevel accuracy    :  3.35 %\n",
      "============================================\n",
      "\n",
      "           tratnsfer    ater                  (gold: transfer)\n",
      "                gola    ald                   (gold: goal)\n",
      "              accmes    acce                  (gold: access)\n",
      "                 gvk    npn                   (gold: gb)\n",
      "                  pt    pat                   (gold: vt)\n",
      "                 psh    she                   (gold: ps)\n",
      "            cotninue    oninue                (gold: continue)\n",
      "                   w    res                   (gold: as)\n",
      "                  eh    esh                   (gold: the)\n",
      "              nekded    need                  (gold: needed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# CIMTransformer   E V A L U A T I O N \n",
    "# ----------------------------------------------------------------------------------\n",
    "import os, math, json, random, torch, transformers, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "#  CONFIG  edit paths / parameters here only --------------------------------\n",
    "eval_args = SimpleNamespace(\n",
    "    test_file      = r\"data/mimic_synthetic/test2k.tsv\",\n",
    "    bert_dir       = r\"bert/ncbi_bert_base\",\n",
    "    ckpt_path      = r\"results/cim_base/ckpt-45000.pkl\",\n",
    "    batch_size     = 32,          # teacherforcing batchsize\n",
    "    num_beams      = 5,\n",
    "    max_gen_length = 64,\n",
    "    ed_weight      = 5.0,         \n",
    "    decoder_layers = 12,\n",
    "    train_bert     = True,\n",
    "    bert_finetune_factor = 1.0,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" device:\", device)\n",
    "\n",
    "#   TOKENISERS ---------------------------------------------------------------\n",
    "bert_tok = CharacterLanguageModel.get_bert_tokenizer(\n",
    "    os.path.join(eval_args.bert_dir, \"vocab.txt\"))\n",
    "typo_tok = CharTokenizer()\n",
    "\n",
    "#   DATASET / DATALOADER -----------------------------------------------------\n",
    "print(\" loading test set \")\n",
    "test_set = TypoDataset(eval_args.test_file, bert_tok, typo_tok, num_process=1)\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=eval_args.batch_size,\n",
    "    shuffle=False, num_workers=0,\n",
    "    collate_fn=test_set.get_collate_fn())\n",
    "print(f\"  {len(test_set):,} examples\")\n",
    "\n",
    "#   (RE)BUILD MODEL & LOAD CHECKPOINT ----------------------------------------\n",
    "bert_cfg_file = os.path.join(eval_args.bert_dir, \"bert_config.json\")\n",
    "bert_cfg  = CharacterLanguageModel.get_bert_config(bert_cfg_file)\n",
    "dec_cfg   = CharacterLanguageModel.get_char_decoder_config(bert_cfg_file, eval_args)\n",
    "\n",
    "bert_m = CharacterLanguageModel.build_bert_model(bert_cfg)\n",
    "bert_m.load_tf_weights(\n",
    "    config            = bert_cfg,\n",
    "    tf_checkpoint_path= os.path.join(eval_args.bert_dir, \"bert_model.ckpt\")\n",
    ")\n",
    "\n",
    "char_emb = CharacterLanguageModel.get_char_embeddings_from_bert(\n",
    "    bert_m.cls.predictions.decoder.weight, bert_tok)\n",
    "char_dec = CharacterLanguageModel.build_char_decoder(dec_cfg, char_emb)\n",
    "\n",
    "# let the model know the chartokenizer\n",
    "eval_args.typo_tokenizer = typo_tok\n",
    "model = CharacterLanguageModel(eval_args, dec_cfg, bert_m, char_dec)\n",
    "\n",
    "ckpt = torch.load(eval_args.ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device).eval()\n",
    "print(f\"  checkpoint loaded: {eval_args.ckpt_path} (step {ckpt.get('step','?')})\")\n",
    "\n",
    "#  LOSS that IGNORES <pad> ---------------------------------------------------\n",
    "class CELossIgnorePad(nn.Module):\n",
    "    def __init__(self, pad_id:int):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "    def forward(self, logits, targets):\n",
    "        V = logits.size(-1)\n",
    "        return F.cross_entropy(\n",
    "            logits.reshape(-1, V), targets.reshape(-1),\n",
    "            ignore_index=self.pad_id, reduction=\"sum\")\n",
    "loss_fn = CELossIgnorePad(pad_id=typo_tok.pad_index)\n",
    "\n",
    "#  EVALUATION LOOP ----------------------------------------------------------\n",
    "loss_sum = tok_corr = tok_total = 0\n",
    "pred_words, gold_words, typo_words = [], [], []\n",
    "\n",
    "print(\" evaluating \")\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_loader, desc=\"eval TFloss\"):\n",
    "        ctx_ids  = batch[\"context_tokens\"].to(device)\n",
    "        ctx_mask = batch[\"context_attention_mask\"].to(device)\n",
    "        tgt_ids  = batch[\"correct_token_ids\"].to(device)\n",
    "        tgt_mask = batch[\"correct_attention_mask\"].to(device).bool()\n",
    "\n",
    "        # ------------ teacherforcing loss / token accuracy ------------------\n",
    "        dec_in  = tgt_ids[:, :-1]\n",
    "        labels  = tgt_ids[:, 1:]\n",
    "        lbl_msk = tgt_mask[:, 1:]\n",
    "\n",
    "        logits = model(input_ids              = ctx_ids,\n",
    "                       attention_mask         = ctx_mask,\n",
    "                       decoder_input_ids      = dec_in,\n",
    "                       decoder_attention_mask = lbl_msk,\n",
    "                       target_correct         = labels).logits\n",
    "\n",
    "        loss_sum  += loss_fn(logits, labels).item()\n",
    "        preds      = logits.argmax(-1)\n",
    "        tok_corr  += (preds.eq(labels) & lbl_msk).sum().item()\n",
    "        tok_total += lbl_msk.sum().item()\n",
    "\n",
    "        # ------------ editdistance beam search (one example at a time) ------\n",
    "        for i in range(ctx_ids.size(0)):\n",
    "            typo  = batch[\"typo\"][i]\n",
    "            gold  = batch[\"correct\"][i]\n",
    "            corr  = generate_with_edit_distance(\n",
    "                        model,\n",
    "                        ctx_ids[i:i+1], ctx_mask[i:i+1],\n",
    "                        typo,\n",
    "                        beam_size = eval_args.num_beams,\n",
    "                        C         = eval_args.ed_weight,\n",
    "                        max_length= eval_args.max_gen_length,\n",
    "                        device    = device)\n",
    "            typo_words.append(typo)\n",
    "            gold_words.append(gold)\n",
    "            pred_words.append(corr)\n",
    "\n",
    "#  REPORT -------------------------------------------------------------------\n",
    "avg_loss = loss_sum / tok_total\n",
    "token_acc = tok_corr / tok_total\n",
    "word_acc  = np.mean([p == g for p, g in zip(pred_words, gold_words)])\n",
    "\n",
    "print(\"\\n================  RESULTS  =================\")\n",
    "print(f\"tokens evaluated (TF)  : {tok_total:,}\")\n",
    "print(f\"average CEloss        : {avg_loss:.4f}\")\n",
    "print(f\"token accuracy (TF)    : {token_acc*100:5.2f} %\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(f\"wordlevel accuracy    : {word_acc*100:5.2f} %\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#  SHOW A FEW RANDOM EXAMPLES ----------------------------------------------\n",
    "for idx in random.sample(range(len(pred_words)), 10):\n",
    "    print(f\"{typo_words[idx]:>20s}    {pred_words[idx]:<20s}  (gold: {gold_words[idx]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9473d63-0562-4285-baa4-4307f055b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " device: cuda\n",
      " loading test set \n",
      "2000 rows data/mimic_synthetic/test2k.tsv... \n",
      "Parsing rows using 1 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2000/2000 [00:06<00:00, 322.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2,000 examples\n",
      "  checkpoint loaded: results/cim_base/ckpt-40000.pkl (step 40000)\n",
      " evaluating \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval TFloss: 100%|| 63/63 [08:27<00:00,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================  RESULTS  =================\n",
      "tokens evaluated (TF)  : 12,456\n",
      "average CEloss        : 3.1125\n",
      "token accuracy (TF)    : 21.07 %\n",
      "--------------------------------------------\n",
      "wordlevel accuracy    :  4.80 %\n",
      "============================================\n",
      "\n",
      "               seace    seace                 (gold: space)\n",
      "                 qni    onir                  (gold: in)\n",
      "                yvwn    yean                  (gold: down)\n",
      "                pass    pass                  (gold: pass)\n",
      "                lzon    ond                   (gold: low)\n",
      "                  pc    pace                  (gold: pcp)\n",
      "               hauev    hed                   (gold: have)\n",
      "                 onw    ond                   (gold: now)\n",
      "                  wo    owo                   (gold: no)\n",
      "              bliatu    bitu                  (gold: bilat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# CIMTransformer   E V A L U A T I O N\n",
    "# ----------------------------------------------------------------------------------\n",
    "import os, math, json, random, torch, transformers, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "#  CONFIG  edit paths / parameters here only --------------------------------\n",
    "eval_args = SimpleNamespace(\n",
    "    test_file      = r\"data/mimic_synthetic/test2k.tsv\",\n",
    "    bert_dir       = r\"bert/ncbi_bert_base\",\n",
    "    ckpt_path      = r\"results/cim_base/ckpt-40000.pkl\",\n",
    "    batch_size     = 32,          # teacherforcing batchsize\n",
    "    num_beams      = 5,\n",
    "    max_gen_length = 64,\n",
    "    ed_weight      = 5.0,         # C in Eq.8\n",
    "    decoder_layers = 12,\n",
    "    train_bert     = True,\n",
    "    bert_finetune_factor = 1.0,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" device:\", device)\n",
    "\n",
    "#  TOKENISERS ---------------------------------------------------------------\n",
    "bert_tok = CharacterLanguageModel.get_bert_tokenizer(\n",
    "    os.path.join(eval_args.bert_dir, \"vocab.txt\"))\n",
    "typo_tok = CharTokenizer()\n",
    "\n",
    "#  DATASET / DATALOADER -----------------------------------------------------\n",
    "print(\" loading test set \")\n",
    "test_set = TypoDataset(eval_args.test_file, bert_tok, typo_tok, num_process=1)\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=eval_args.batch_size,\n",
    "    shuffle=False, num_workers=0,\n",
    "    collate_fn=test_set.get_collate_fn())\n",
    "print(f\"  {len(test_set):,} examples\")\n",
    "\n",
    "#  (RE)BUILD MODEL & LOAD CHECKPOINT ----------------------------------------\n",
    "bert_cfg_file = os.path.join(eval_args.bert_dir, \"bert_config.json\")\n",
    "bert_cfg  = CharacterLanguageModel.get_bert_config(bert_cfg_file)\n",
    "dec_cfg   = CharacterLanguageModel.get_char_decoder_config(bert_cfg_file, eval_args)\n",
    "\n",
    "bert_m = CharacterLanguageModel.build_bert_model(bert_cfg)\n",
    "bert_m.load_tf_weights(\n",
    "    config            = bert_cfg,\n",
    "    tf_checkpoint_path= os.path.join(eval_args.bert_dir, \"bert_model.ckpt\")\n",
    ")\n",
    "\n",
    "char_emb = CharacterLanguageModel.get_char_embeddings_from_bert(\n",
    "    bert_m.cls.predictions.decoder.weight, bert_tok)\n",
    "char_dec = CharacterLanguageModel.build_char_decoder(dec_cfg, char_emb)\n",
    "\n",
    "# let the model know the chartokenizer\n",
    "eval_args.typo_tokenizer = typo_tok\n",
    "model = CharacterLanguageModel(eval_args, dec_cfg, bert_m, char_dec)\n",
    "\n",
    "ckpt = torch.load(eval_args.ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device).eval()\n",
    "print(f\"  checkpoint loaded: {eval_args.ckpt_path} (step {ckpt.get('step','?')})\")\n",
    "\n",
    "#  LOSS that IGNORES <pad> ---------------------------------------------------\n",
    "class CELossIgnorePad(nn.Module):\n",
    "    def __init__(self, pad_id:int):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "    def forward(self, logits, targets):\n",
    "        V = logits.size(-1)\n",
    "        return F.cross_entropy(\n",
    "            logits.reshape(-1, V), targets.reshape(-1),\n",
    "            ignore_index=self.pad_id, reduction=\"sum\")\n",
    "loss_fn = CELossIgnorePad(pad_id=typo_tok.pad_index)\n",
    "\n",
    "#  EVALUATION LOOP ----------------------------------------------------------\n",
    "loss_sum = tok_corr = tok_total = 0\n",
    "pred_words, gold_words, typo_words = [], [], []\n",
    "\n",
    "print(\" evaluating \")\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_loader, desc=\"eval TFloss\"):\n",
    "        ctx_ids  = batch[\"context_tokens\"].to(device)\n",
    "        ctx_mask = batch[\"context_attention_mask\"].to(device)\n",
    "        tgt_ids  = batch[\"correct_token_ids\"].to(device)\n",
    "        tgt_mask = batch[\"correct_attention_mask\"].to(device).bool()\n",
    "\n",
    "        # ------------ teacherforcing loss / token accuracy ------------------\n",
    "        dec_in  = tgt_ids[:, :-1]\n",
    "        labels  = tgt_ids[:, 1:]\n",
    "        lbl_msk = tgt_mask[:, 1:]\n",
    "\n",
    "        logits = model(input_ids              = ctx_ids,\n",
    "                       attention_mask         = ctx_mask,\n",
    "                       decoder_input_ids      = dec_in,\n",
    "                       decoder_attention_mask = lbl_msk,\n",
    "                       target_correct         = labels).logits\n",
    "\n",
    "        loss_sum  += loss_fn(logits, labels).item()\n",
    "        preds      = logits.argmax(-1)\n",
    "        tok_corr  += (preds.eq(labels) & lbl_msk).sum().item()\n",
    "        tok_total += lbl_msk.sum().item()\n",
    "\n",
    "        # ------------ editdistance beam search (one example at a time) ------\n",
    "        for i in range(ctx_ids.size(0)):\n",
    "            typo  = batch[\"typo\"][i]\n",
    "            gold  = batch[\"correct\"][i]\n",
    "            corr  = generate_with_edit_distance(\n",
    "                        model,\n",
    "                        ctx_ids[i:i+1], ctx_mask[i:i+1],\n",
    "                        typo,\n",
    "                        beam_size = eval_args.num_beams,\n",
    "                        C         = eval_args.ed_weight,\n",
    "                        max_length= eval_args.max_gen_length,\n",
    "                        device    = device)\n",
    "            typo_words.append(typo)\n",
    "            gold_words.append(gold)\n",
    "            pred_words.append(corr)\n",
    "\n",
    "#  REPORT -------------------------------------------------------------------\n",
    "avg_loss = loss_sum / tok_total\n",
    "token_acc = tok_corr / tok_total\n",
    "word_acc  = np.mean([p == g for p, g in zip(pred_words, gold_words)])\n",
    "\n",
    "print(\"\\n================  RESULTS  =================\")\n",
    "print(f\"tokens evaluated (TF)  : {tok_total:,}\")\n",
    "print(f\"average CEloss        : {avg_loss:.4f}\")\n",
    "print(f\"token accuracy (TF)    : {token_acc*100:5.2f} %\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(f\"wordlevel accuracy    : {word_acc*100:5.2f} %\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#  SHOW A FEW RANDOM EXAMPLES ----------------------------------------------\n",
    "for idx in random.sample(range(len(pred_words)), 10):\n",
    "    print(f\"{typo_words[idx]:>20s}    {pred_words[idx]:<20s}  (gold: {gold_words[idx]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d83a47e5-9c3a-4ce1-a971-898924f9ade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " device: cuda\n",
      " loading test set \n",
      "2000 rows data/mimic_synthetic/test2k.tsv... \n",
      "Parsing rows using 1 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2000/2000 [00:06<00:00, 321.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2,000 examples\n",
      "  checkpoint loaded: results/cim_base/ckpt-45000.pkl (step 45000)\n",
      " evaluating \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval TFloss: 100%|| 63/63 [09:16<00:00,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================  RESULTS  =================\n",
      "tokens evaluated (TF)  : 12,456\n",
      "average CEloss        : 3.3905\n",
      "token accuracy (TF)    : 17.90 %\n",
      "--------------------------------------------\n",
      "wordlevel accuracy    :  2.65 %\n",
      "============================================\n",
      "\n",
      "                  no    conot                 (gold: not)\n",
      "                   m    emp                   (gold: mg)\n",
      "          dentitionz    denitin               (gold: dentition)\n",
      "                 oct    cort                  (gold: cont)\n",
      "                 ecs    escp                  (gold: sec)\n",
      "                mtcx    ctan                  (gold: tmax)\n",
      "                  zt    att                   (gold: to)\n",
      "             ovrload    lad                   (gold: overload)\n",
      "                   b    bone                  (gold: by)\n",
      "           coxonarpy    conary                (gold: coronary)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# CIMTransformer   E V A L U A T I O N\n",
    "# ----------------------------------------------------------------------------------\n",
    "import os, math, json, random, torch, transformers, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "#  CONFIG  edit paths / parameters here only --------------------------------\n",
    "eval_args = SimpleNamespace(\n",
    "    test_file      = r\"data/mimic_synthetic/test2k.tsv\",\n",
    "    bert_dir       = r\"bert/ncbi_bert_base\",\n",
    "    ckpt_path      = r\"results/cim_base/ckpt-45000.pkl\",\n",
    "    batch_size     = 32,          \n",
    "    num_beams      = 5,\n",
    "    max_gen_length = 64,\n",
    "    ed_weight      = 5.0,         \n",
    "    decoder_layers = 12,\n",
    "    train_bert     = True,\n",
    "    bert_finetune_factor = 1.0,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" device:\", device)\n",
    "\n",
    "#  TOKENISERS ---------------------------------------------------------------\n",
    "bert_tok = CharacterLanguageModel.get_bert_tokenizer(\n",
    "    os.path.join(eval_args.bert_dir, \"vocab.txt\"))\n",
    "typo_tok = CharTokenizer()\n",
    "\n",
    "#  DATASET / DATALOADER -----------------------------------------------------\n",
    "print(\" loading test set \")\n",
    "test_set = TypoDataset(eval_args.test_file, bert_tok, typo_tok, num_process=1)\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=eval_args.batch_size,\n",
    "    shuffle=False, num_workers=0,\n",
    "    collate_fn=test_set.get_collate_fn())\n",
    "print(f\"  {len(test_set):,} examples\")\n",
    "\n",
    "#  (RE)BUILD MODEL & LOAD CHECKPOINT ----------------------------------------\n",
    "bert_cfg_file = os.path.join(eval_args.bert_dir, \"bert_config.json\")\n",
    "bert_cfg  = CharacterLanguageModel.get_bert_config(bert_cfg_file)\n",
    "dec_cfg   = CharacterLanguageModel.get_char_decoder_config(bert_cfg_file, eval_args)\n",
    "\n",
    "bert_m = CharacterLanguageModel.build_bert_model(bert_cfg)\n",
    "bert_m.load_tf_weights(\n",
    "    config            = bert_cfg,\n",
    "    tf_checkpoint_path= os.path.join(eval_args.bert_dir, \"bert_model.ckpt\")\n",
    ")\n",
    "\n",
    "char_emb = CharacterLanguageModel.get_char_embeddings_from_bert(\n",
    "    bert_m.cls.predictions.decoder.weight, bert_tok)\n",
    "char_dec = CharacterLanguageModel.build_char_decoder(dec_cfg, char_emb)\n",
    "\n",
    "# let the model know the chartokenizer\n",
    "eval_args.typo_tokenizer = typo_tok\n",
    "model = CharacterLanguageModel(eval_args, dec_cfg, bert_m, char_dec)\n",
    "\n",
    "ckpt = torch.load(eval_args.ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device).eval()\n",
    "print(f\"  checkpoint loaded: {eval_args.ckpt_path} (step {ckpt.get('step','?')})\")\n",
    "\n",
    "#  LOSS that IGNORES <pad> ---------------------------------------------------\n",
    "class CELossIgnorePad(nn.Module):\n",
    "    def __init__(self, pad_id:int):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "    def forward(self, logits, targets):\n",
    "        V = logits.size(-1)\n",
    "        return F.cross_entropy(\n",
    "            logits.reshape(-1, V), targets.reshape(-1),\n",
    "            ignore_index=self.pad_id, reduction=\"sum\")\n",
    "loss_fn = CELossIgnorePad(pad_id=typo_tok.pad_index)\n",
    "\n",
    "#  EVALUATION LOOP ----------------------------------------------------------\n",
    "loss_sum = tok_corr = tok_total = 0\n",
    "pred_words, gold_words, typo_words = [], [], []\n",
    "\n",
    "print(\" evaluating \")\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_loader, desc=\"eval TFloss\"):\n",
    "        ctx_ids  = batch[\"context_tokens\"].to(device)\n",
    "        ctx_mask = batch[\"context_attention_mask\"].to(device)\n",
    "        tgt_ids  = batch[\"correct_token_ids\"].to(device)\n",
    "        tgt_mask = batch[\"correct_attention_mask\"].to(device).bool()\n",
    "\n",
    "        # ------------ teacherforcing loss / token accuracy ------------------\n",
    "        dec_in  = tgt_ids[:, :-1]\n",
    "        labels  = tgt_ids[:, 1:]\n",
    "        lbl_msk = tgt_mask[:, 1:]\n",
    "\n",
    "        logits = model(input_ids              = ctx_ids,\n",
    "                       attention_mask         = ctx_mask,\n",
    "                       decoder_input_ids      = dec_in,\n",
    "                       decoder_attention_mask = lbl_msk,\n",
    "                       target_correct         = labels).logits\n",
    "\n",
    "        loss_sum  += loss_fn(logits, labels).item()\n",
    "        preds      = logits.argmax(-1)\n",
    "        tok_corr  += (preds.eq(labels) & lbl_msk).sum().item()\n",
    "        tok_total += lbl_msk.sum().item()\n",
    "\n",
    "        # ------------ editdistance beam search (one example at a time) ------\n",
    "        for i in range(ctx_ids.size(0)):\n",
    "            typo  = batch[\"typo\"][i]\n",
    "            gold  = batch[\"correct\"][i]\n",
    "            corr  = generate_with_edit_distance(\n",
    "                        model,\n",
    "                        ctx_ids[i:i+1], ctx_mask[i:i+1],\n",
    "                        typo,\n",
    "                        beam_size = eval_args.num_beams,\n",
    "                        C         = eval_args.ed_weight,\n",
    "                        max_length= eval_args.max_gen_length,\n",
    "                        device    = device)\n",
    "            typo_words.append(typo)\n",
    "            gold_words.append(gold)\n",
    "            pred_words.append(corr)\n",
    "\n",
    "#  REPORT -------------------------------------------------------------------\n",
    "avg_loss = loss_sum / tok_total\n",
    "token_acc = tok_corr / tok_total\n",
    "word_acc  = np.mean([p == g for p, g in zip(pred_words, gold_words)])\n",
    "\n",
    "print(\"\\n================  RESULTS  =================\")\n",
    "print(f\"tokens evaluated (TF)  : {tok_total:,}\")\n",
    "print(f\"average CEloss        : {avg_loss:.4f}\")\n",
    "print(f\"token accuracy (TF)    : {token_acc*100:5.2f} %\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(f\"wordlevel accuracy    : {word_acc*100:5.2f} %\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#  SHOW A FEW RANDOM EXAMPLES ----------------------------------------------\n",
    "for idx in random.sample(range(len(pred_words)), 10):\n",
    "    print(f\"{typo_words[idx]:>20s}    {pred_words[idx]:<20s}  (gold: {gold_words[idx]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed863917-9a21-4806-aba1-2970880ee33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " device: cuda\n",
      " loading test set \n",
      "2000 rows data/mimic_synthetic/test2k.tsv... \n",
      "Parsing rows using 1 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2000/2000 [00:06<00:00, 322.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2,000 examples\n",
      "  checkpoint loaded: results/cim_base/ckpt-60000.pkl (step 60000)\n",
      " evaluating \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval TFloss: 100%|| 63/63 [08:09<00:00,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================  RESULTS  =================\n",
      "tokens evaluated (TF)  : 12,456\n",
      "average CEloss        : 3.2511\n",
      "token accuracy (TF)    : 20.20 %\n",
      "--------------------------------------------\n",
      "wordlevel accuracy    :  3.85 %\n",
      "============================================\n",
      "\n",
      "             ventrla    ven                   (gold: ventral)\n",
      "             pljease    pas                   (gold: please)\n",
      "                 bsm    semal                 (gold: bms)\n",
      "             vimting    min                   (gold: vomiting)\n",
      "                 ntw    tonte                 (gold: new)\n",
      "            infudons    fons                  (gold: infusions)\n",
      "                 hte    the                   (gold: the)\n",
      "                  re    fem                   (gold: he)\n",
      "               psint    sint                  (gold: point)\n",
      "             efusion    iond                  (gold: effusion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# CIMTransformer   E V A L U A T I O N  \n",
    "# ----------------------------------------------------------------------------------\n",
    "import os, math, json, random, torch, transformers, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "#  CONFIG  edit paths / parameters here only --------------------------------\n",
    "eval_args = SimpleNamespace(\n",
    "    test_file      = r\"data/mimic_synthetic/test2k.tsv\",\n",
    "    bert_dir       = r\"bert/ncbi_bert_base\",\n",
    "    ckpt_path      = r\"results/cim_base/ckpt-60000.pkl\",\n",
    "    batch_size     = 32,          \n",
    "    num_beams      = 5,\n",
    "    max_gen_length = 64,\n",
    "    ed_weight      = 5.0,         \n",
    "    decoder_layers = 12,\n",
    "    train_bert     = True,\n",
    "    bert_finetune_factor = 1.0,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" device:\", device)\n",
    "\n",
    "#  TOKENISERS ---------------------------------------------------------------\n",
    "bert_tok = CharacterLanguageModel.get_bert_tokenizer(\n",
    "    os.path.join(eval_args.bert_dir, \"vocab.txt\"))\n",
    "typo_tok = CharTokenizer()\n",
    "\n",
    "#  DATASET / DATALOADER -----------------------------------------------------\n",
    "print(\" loading test set \")\n",
    "test_set = TypoDataset(eval_args.test_file, bert_tok, typo_tok, num_process=1)\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=eval_args.batch_size,\n",
    "    shuffle=False, num_workers=0,\n",
    "    collate_fn=test_set.get_collate_fn())\n",
    "print(f\"  {len(test_set):,} examples\")\n",
    "\n",
    "#  (RE)BUILD MODEL & LOAD CHECKPOINT ----------------------------------------\n",
    "bert_cfg_file = os.path.join(eval_args.bert_dir, \"bert_config.json\")\n",
    "bert_cfg  = CharacterLanguageModel.get_bert_config(bert_cfg_file)\n",
    "dec_cfg   = CharacterLanguageModel.get_char_decoder_config(bert_cfg_file, eval_args)\n",
    "\n",
    "bert_m = CharacterLanguageModel.build_bert_model(bert_cfg)\n",
    "bert_m.load_tf_weights(\n",
    "    config            = bert_cfg,\n",
    "    tf_checkpoint_path= os.path.join(eval_args.bert_dir, \"bert_model.ckpt\")\n",
    ")\n",
    "\n",
    "char_emb = CharacterLanguageModel.get_char_embeddings_from_bert(\n",
    "    bert_m.cls.predictions.decoder.weight, bert_tok)\n",
    "char_dec = CharacterLanguageModel.build_char_decoder(dec_cfg, char_emb)\n",
    "\n",
    "# let the model know the chartokenizer\n",
    "eval_args.typo_tokenizer = typo_tok\n",
    "model = CharacterLanguageModel(eval_args, dec_cfg, bert_m, char_dec)\n",
    "\n",
    "ckpt = torch.load(eval_args.ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device).eval()\n",
    "print(f\"  checkpoint loaded: {eval_args.ckpt_path} (step {ckpt.get('step','?')})\")\n",
    "\n",
    "#  LOSS that IGNORES <pad> ---------------------------------------------------\n",
    "class CELossIgnorePad(nn.Module):\n",
    "    def __init__(self, pad_id:int):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "    def forward(self, logits, targets):\n",
    "        V = logits.size(-1)\n",
    "        return F.cross_entropy(\n",
    "            logits.reshape(-1, V), targets.reshape(-1),\n",
    "            ignore_index=self.pad_id, reduction=\"sum\")\n",
    "loss_fn = CELossIgnorePad(pad_id=typo_tok.pad_index)\n",
    "\n",
    "#  EVALUATION LOOP ----------------------------------------------------------\n",
    "loss_sum = tok_corr = tok_total = 0\n",
    "pred_words, gold_words, typo_words = [], [], []\n",
    "\n",
    "print(\" evaluating \")\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_loader, desc=\"eval TFloss\"):\n",
    "        ctx_ids  = batch[\"context_tokens\"].to(device)\n",
    "        ctx_mask = batch[\"context_attention_mask\"].to(device)\n",
    "        tgt_ids  = batch[\"correct_token_ids\"].to(device)\n",
    "        tgt_mask = batch[\"correct_attention_mask\"].to(device).bool()\n",
    "\n",
    "        # ------------ teacherforcing loss / token accuracy ------------------\n",
    "        dec_in  = tgt_ids[:, :-1]\n",
    "        labels  = tgt_ids[:, 1:]\n",
    "        lbl_msk = tgt_mask[:, 1:]\n",
    "\n",
    "        logits = model(input_ids              = ctx_ids,\n",
    "                       attention_mask         = ctx_mask,\n",
    "                       decoder_input_ids      = dec_in,\n",
    "                       decoder_attention_mask = lbl_msk,\n",
    "                       target_correct         = labels).logits\n",
    "\n",
    "        loss_sum  += loss_fn(logits, labels).item()\n",
    "        preds      = logits.argmax(-1)\n",
    "        tok_corr  += (preds.eq(labels) & lbl_msk).sum().item()\n",
    "        tok_total += lbl_msk.sum().item()\n",
    "\n",
    "        # ------------ editdistance beam search (one example at a time) ------\n",
    "        for i in range(ctx_ids.size(0)):\n",
    "            typo  = batch[\"typo\"][i]\n",
    "            gold  = batch[\"correct\"][i]\n",
    "            corr  = generate_with_edit_distance(\n",
    "                        model,\n",
    "                        ctx_ids[i:i+1], ctx_mask[i:i+1],\n",
    "                        typo,\n",
    "                        beam_size = eval_args.num_beams,\n",
    "                        C         = eval_args.ed_weight,\n",
    "                        max_length= eval_args.max_gen_length,\n",
    "                        device    = device)\n",
    "            typo_words.append(typo)\n",
    "            gold_words.append(gold)\n",
    "            pred_words.append(corr)\n",
    "\n",
    "#  REPORT -------------------------------------------------------------------\n",
    "avg_loss = loss_sum / tok_total\n",
    "token_acc = tok_corr / tok_total\n",
    "word_acc  = np.mean([p == g for p, g in zip(pred_words, gold_words)])\n",
    "\n",
    "print(\"\\n================  RESULTS  =================\")\n",
    "print(f\"tokens evaluated (TF)  : {tok_total:,}\")\n",
    "print(f\"average CEloss        : {avg_loss:.4f}\")\n",
    "print(f\"token accuracy (TF)    : {token_acc*100:5.2f} %\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(f\"wordlevel accuracy    : {word_acc*100:5.2f} %\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "#  SHOW A FEW RANDOM EXAMPLES ----------------------------------------------\n",
    "for idx in random.sample(range(len(pred_words)), 10):\n",
    "    print(f\"{typo_words[idx]:>20s}    {pred_words[idx]:<20s}  (gold: {gold_words[idx]})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dfd90e9-b0bb-4a6d-b39c-6787fe1b9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir   = \"results/cim_base\"   \n",
    "ckpt_paths = glob.glob(os.path.join(ckpt_dir, \"ckpt-*.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6523855b-375e-410a-a959-ea09ad7280ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file data/mimic_synthetic\\test2k.tsv... 2000 rows\n",
      "Parsing rows using 1 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2000/2000 [00:06<00:00, 323.77it/s]\n",
      "Evaluating checkpoints:  20%|        | 1/5 [07:23<29:34, 443.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   5000  wordaccuracy:  3.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating checkpoints:  40%|      | 2/5 [16:29<25:11, 503.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  45000  wordaccuracy:  2.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating checkpoints:  60%|    | 3/5 [25:41<17:31, 525.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  50000  wordaccuracy:  2.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating checkpoints:  80%|  | 4/5 [34:46<08:53, 533.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  55000  wordaccuracy:  1.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating checkpoints: 100%|| 5/5 [42:56<00:00, 515.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  60000  wordaccuracy:  3.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  1) Rebuild the test DataLoader \n",
    "device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_tok    = CharacterLanguageModel.get_bert_tokenizer(os.path.join(args.bert_dir, \"vocab.txt\"))\n",
    "typo_tok    = CharTokenizer()\n",
    "test_set    = TypoDataset(os.path.join(args.data_dir, \"test2k.tsv\"), bert_tok, typo_tok, num_process=1)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=test_set.get_collate_fn()\n",
    ")\n",
    "\n",
    "#  2) Helper to extract step number from filename \n",
    "def step_from_path(path):\n",
    "    m = re.search(r\"ckpt-(\\d+)\\.pkl$\", path)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "#  3) Find all checkpoints \n",
    "ckpt_dir   = os.path.join(\"results\", \"cim_base\")\n",
    "ckpt_paths = sorted(glob.glob(os.path.join(ckpt_dir, \"ckpt-*.pkl\")), key=step_from_path)\n",
    "\n",
    "steps, accs = [], []\n",
    "\n",
    "#  4) Evaluate each checkpoint \n",
    "for ckpt_path in tqdm(ckpt_paths, desc=\"Evaluating checkpoints\"):\n",
    "    step = step_from_path(ckpt_path)\n",
    "    # load weights\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model.to(device).eval()\n",
    "\n",
    "    golds, preds = [], []\n",
    "    with torch.inference_mode():\n",
    "        for batch in test_loader:\n",
    "            ctx_ids  = batch[\"context_tokens\"].to(device)\n",
    "            ctx_mask = batch[\"context_attention_mask\"].to(device)\n",
    "            for i in range(ctx_ids.size(0)):\n",
    "                typo = batch[\"typo\"][i]\n",
    "                gold = batch[\"correct\"][i]\n",
    "                pred = generate_with_edit_distance(\n",
    "                    model,\n",
    "                    ctx_ids[i:i+1], ctx_mask[i:i+1],\n",
    "                    typo,\n",
    "                    beam_size=args.num_beams,\n",
    "                    C=args.edit_distance_weight,\n",
    "                    max_length=64,\n",
    "                    device=device\n",
    "                )\n",
    "                golds.append(gold)\n",
    "                preds.append(pred)\n",
    "\n",
    "    word_acc = np.mean([p == g for p, g in zip(preds, golds)])\n",
    "    steps.append(step)\n",
    "    accs.append(word_acc)\n",
    "    print(f\"Step {step:>6}  wordaccuracy: {word_acc*100:5.2f}%\")\n",
    "\n",
    "#  5) Plot accuracy vs. training step \n",
    "plt.figure()\n",
    "plt.plot(steps, accs, marker=\"o\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"WordLevel Accuracy\")\n",
    "plt.title(\"Checkpoint Performance on Test Set\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3f117-668e-4a21-a919-b2329c861acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
