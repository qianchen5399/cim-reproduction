{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2913ee9f-916f-4688-b2f5-51c715fee6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4849dbb2-de04-4b84-a57c-9ddb4871878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant paths (adjust as needed)\n",
    "MIMIC_DIR = '../data/mimic3'\n",
    "NOTEEVENTS_FNAME = 'NOTEEVENTS.csv'\n",
    "NOTE_SPLIT_DIR = os.path.join(MIMIC_DIR, 'split')\n",
    "NOTE_PER_SPLIT = 100000  # Number of rows per split\n",
    "\n",
    "ENGLISH_WORDS_FPATH = '../data/english_words/words.txt'\n",
    "UMLS_DIR = '../data/umls'\n",
    "LEXICON_OUT_DIR = '../data/lexicon'\n",
    "LEXICON_OUT_FNAME = 'lexicon.json'\n",
    "\n",
    "# For filtering\n",
    "ALPHA_SET = set(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "ALLOWED_CHAR_SET = set(\"abcdefghijklmnopqrstuvwxyz'-&\")\n",
    "CHAR_SET = set(\"0123456789abcdefghijklmnopqrstuvwxyz+-*/^.,;:=!?'()[]{} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314940fc-2cc9-42d6-8044-f9b3d89d6080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/mimic3\\NOTEEVENTS.csv contains 2083180 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_0.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_1.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_2.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_3.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_4.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_5.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_6.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_7.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_8.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_9.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_10.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_11.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_12.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_13.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_14.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_15.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_16.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_17.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_18.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_19.csv with 100000 rows\n",
      "Created ../data/mimic3\\split\\NOTEEVENTS_20.csv with 83180 rows\n"
     ]
    }
   ],
   "source": [
    "# Split the NOTEEVENTS.csv into smaller files\n",
    "\n",
    "orig_note_fpath = os.path.join(MIMIC_DIR, NOTEEVENTS_FNAME)\n",
    "\n",
    "# Load the original MIMIC-III note file\n",
    "df_note = pd.read_csv(orig_note_fpath, low_memory=False)\n",
    "print(f\"{orig_note_fpath} contains {len(df_note)} rows\")\n",
    "\n",
    "# Create output dir if it doesn't exist\n",
    "if not os.path.exists(NOTE_SPLIT_DIR):\n",
    "    os.makedirs(NOTE_SPLIT_DIR)\n",
    "\n",
    "# Split and save\n",
    "for i in range(0, len(df_note), NOTE_PER_SPLIT):\n",
    "    split_chunk = df_note[i:i+NOTE_PER_SPLIT]\n",
    "    split_fpath = os.path.join(NOTE_SPLIT_DIR, f'NOTEEVENTS_{i//NOTE_PER_SPLIT}.csv')\n",
    "    split_chunk.to_csv(split_fpath, index=False)\n",
    "    print(f\"Created {split_fpath} with {len(split_chunk)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f36b56c1-112a-42a5-8a23-2087b34dd3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 465091 English words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading LRWD: 100%|██████████| 1304243/1304243 [00:01<00:00, 723352.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRWD has 401471 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading prevariants: 100%|██████████| 923076/923076 [00:02<00:00, 431610.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevariants has 509533 words\n",
      "UMLS total = 514305 words\n",
      "Total vocab = 822919 words\n",
      "Saved final lexicon to ../data/lexicon\\lexicon.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Load English words (with explicit UTF-8, fallback to 'replace' if you want to avoid errors altogether)\n",
    "with open(ENGLISH_WORDS_FPATH, 'r', encoding='utf-8', errors='replace') as fd:\n",
    "    english_vocab = [line.strip().lower() for line in fd]\n",
    "\n",
    "# Filter out words that have only allowed chars and start with alpha\n",
    "english_vocab = [\n",
    "    w for w in english_vocab\n",
    "    if w\n",
    "    and all(c in ALLOWED_CHAR_SET for c in w)\n",
    "    and (w[0] in ALPHA_SET)\n",
    "]\n",
    "english_vocab_set = set(english_vocab)\n",
    "print(f\"Loaded {len(english_vocab_set)} English words\")\n",
    "\n",
    "# 2) Load UMLS LRWD\n",
    "lrwd_vocab_set = set()\n",
    "lrwd_path = os.path.join(UMLS_DIR, 'LRWD')\n",
    "\n",
    "with open(lrwd_path, 'r', encoding='utf-8', errors='replace') as fd:\n",
    "    lines = fd.read().splitlines()  # or fd.readlines()\n",
    "for line in tqdm(lines, desc=\"Loading LRWD\"):\n",
    "    # The original code splits the line into word, eui, temp using '|'\n",
    "    # If your file format differs, handle that here\n",
    "    word, eui, temp = line.split('|')\n",
    "    # word might have spaces, but typically they are single words\n",
    "    if all(c in ALLOWED_CHAR_SET for c in word) and (word[0] in ALPHA_SET):\n",
    "        for splited in word.split():\n",
    "            lrwd_vocab_set.add(splited.lower())\n",
    "\n",
    "print(f\"LRWD has {len(lrwd_vocab_set)} words\")\n",
    "\n",
    "# 3) Load UMLS Prevariants\n",
    "prevariants_vocab_set = set()\n",
    "prevars_path = os.path.join(UMLS_DIR, 'prevariants')\n",
    "\n",
    "with open(prevars_path, 'r', encoding='utf-8', errors='replace') as fd:\n",
    "    lines = fd.read().splitlines()\n",
    "for line in tqdm(lines, desc=\"Loading prevariants\"):\n",
    "    phrase, eui, temp = line.split('|')\n",
    "    for w in phrase.split():\n",
    "        if all(c in ALLOWED_CHAR_SET for c in w) and (w[0] in ALPHA_SET):\n",
    "            prevariants_vocab_set.add(w.lower())\n",
    "\n",
    "print(f\"Prevariants has {len(prevariants_vocab_set)} words\")\n",
    "\n",
    "# Merge them\n",
    "umls_vocab_set = lrwd_vocab_set | prevariants_vocab_set\n",
    "print(f\"UMLS total = {len(umls_vocab_set)} words\")\n",
    "\n",
    "# 4) Create final total vocab\n",
    "total_vocab_set = english_vocab_set | umls_vocab_set\n",
    "total_vocab_list = sorted(list(total_vocab_set))\n",
    "print(f\"Total vocab = {len(total_vocab_list)} words\")\n",
    "\n",
    "# 5) Write out the lexicon (JSON)\n",
    "if not os.path.exists(LEXICON_OUT_DIR):\n",
    "    os.makedirs(LEXICON_OUT_DIR)\n",
    "\n",
    "lexicon_out_fpath = os.path.join(LEXICON_OUT_DIR, LEXICON_OUT_FNAME)\n",
    "with open(lexicon_out_fpath, 'w', encoding='utf-8') as fd:\n",
    "    json.dump(total_vocab_list, fd, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved final lexicon to {lexicon_out_fpath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9430d34-eff8-47a3-9b2d-a32f615bf4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 1580871 tokens. Coverage in lexicon: 69.15%\n"
     ]
    }
   ],
   "source": [
    "# Load the saved lexicon\n",
    "with open(os.path.join(LEXICON_OUT_DIR, LEXICON_OUT_FNAME), 'r', encoding='utf-8') as f:\n",
    "    final_lexicon = set(json.load(f))\n",
    "\n",
    "# Read a portion of a split MIMIC-III file (adjust filename/column if needed)\n",
    "df_sample = pd.read_csv(os.path.join(NOTE_SPLIT_DIR, 'NOTEEVENTS_0.csv'), nrows=1000)\n",
    "text_lines = df_sample['TEXT'].dropna().tolist()\n",
    "\n",
    "# Quick, naive tokenization on whitespace\n",
    "tokens = []\n",
    "for line in text_lines:\n",
    "    tokens.extend(line.lower().split())\n",
    "\n",
    "if tokens:\n",
    "    in_lex = sum(token in final_lexicon for token in tokens)\n",
    "    coverage = in_lex / len(tokens)\n",
    "    print(f\"Checked {len(tokens)} tokens. Coverage in lexicon: {coverage:.2%}\")\n",
    "else:\n",
    "    print(\"No tokens found (column might be empty).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f45fcbbe-5046-4788-a2b5-e0cb5814d1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique token types: 84,206\n",
      "Top 10 tokens by frequency:\n",
      "the: 38562\n",
      "and: 32645\n",
      "to: 28448\n",
      "of: 27545\n",
      "was: 26744\n",
      "with: 19844\n",
      "on: 18046\n",
      "a: 17907\n",
      "in: 15520\n",
      "for: 14022\n",
      "\n",
      "Tokens that appear only once: 44,730\n",
      "\n",
      "Out‑of‑lexicon token types: 68,443\n",
      "Top 20 out‑of‑lexicon tokens:\n",
      "(1): 6775\n",
      "sig:: 6649\n",
      ".: 5967\n",
      "-: 5908\n",
      "**]: 5444\n",
      "[**last: 3887\n",
      "history:: 2812\n",
      "1.: 2710\n",
      "2.: 2656\n",
      "[**hospital1: 2541\n",
      "3.: 2350\n",
      "2: 2235\n",
      "1: 2207\n",
      "#: 2178\n",
      "date:: 2011\n",
      "4.: 1937\n",
      "(stitle): 1906\n",
      "[**first: 1736\n",
      "s/p: 1732\n",
      "(daily).: 1695\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# If you want to keep the original name, do this:\n",
    "# all_tokens = tokens\n",
    "\n",
    "freq_counter = Counter(tokens)          # ← use the list you actually filled\n",
    "print(f\"Unique token types: {len(freq_counter):,}\")\n",
    "\n",
    "most_common_tokens = freq_counter.most_common(10)\n",
    "print(\"Top 10 tokens by frequency:\")\n",
    "for token, count in most_common_tokens:\n",
    "    print(f\"{token}: {count}\")\n",
    "\n",
    "rare_count = sum(1 for t, c in freq_counter.items() if c == 1)\n",
    "print(f\"\\nTokens that appear only once: {rare_count:,}\")\n",
    "\n",
    "oov_list = [t for t in freq_counter if t not in final_lexicon]\n",
    "print(f\"\\nOut‑of‑lexicon token types: {len(oov_list):,}\")\n",
    "\n",
    "oov_sorted = sorted(oov_list, key=lambda x: freq_counter[x], reverse=True)[:20]\n",
    "print(\"Top 20 out‑of‑lexicon tokens:\")\n",
    "for token in oov_sorted:\n",
    "    print(f\"{token}: {freq_counter[token]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d781b4d4-c883-4b7e-bb6f-9a1b358467d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinspell vocabulary size: 293463\n",
      "Clinspell did not cover 1 corrected words:\n",
      "['echinocytes']\n"
     ]
    }
   ],
   "source": [
    "corrected = [\n",
    "    \"cardiology\", \"lungs\", \"lungs\", \"lungs\", \"lungs\", \"lungs\",\n",
    "    \"echinocytes\", \"procedure\", \"procedure\", \"evening\", \"evening\",\n",
    "    \"evening\", \"etanercept\", \"etanercept\", \"etanercept\", \"hepatology\"\n",
    "    # ... more if needed\n",
    "]\n",
    "\n",
    "with open('../data/mimic_clinspell/lexicon_en.json', 'r', encoding='utf-8') as f:\n",
    "    clinspell_list = json.load(f)\n",
    "\n",
    "clinspell_set = set(clinspell_list)\n",
    "print(f\"Clinspell vocabulary size: {len(clinspell_set)}\")\n",
    "\n",
    "missing = [w for w in corrected if w not in clinspell_set]\n",
    "print(f\"Clinspell did not cover {len(missing)} corrected words:\")\n",
    "print(missing[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46b912-5a43-4b5f-856d-5e65015f62b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
